{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f15c158",
   "metadata": {},
   "source": [
    "# Text Summarization Model Training\n",
    "\n",
    "This notebook demonstrates how to fine-tune a transformer model (BART) on the CNN/DailyMail dataset using SageMaker for text summarization.\n",
    "\n",
    "**Note**: This notebook is configured to run with the **Python 3 (ipykernel)** kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd60ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries - expanded to include all necessary dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install boto3 sagemaker\n",
    "!pip install transformers datasets\n",
    "!pip install torch torchvision\n",
    "!pip install pandas numpy scikit-learn\n",
    "!pip install rouge-score nltk\n",
    "\n",
    "# Verify CUDA is not available (CPU training)\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count() if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up working directory for backend files\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Create backend directory if it doesn't exist\n",
    "!mkdir -p backend\n",
    "\n",
    "# If backend files are not already in the notebook instance, we need to create them\n",
    "# Check if train.py exists in backend directory\n",
    "if not os.path.exists('backend/train.py'):\n",
    "    print(\"Creating backend files...\")\n",
    "    # We'll create these files later, but for now let's check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa661e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# Initialize SageMaker session\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define S3 bucket and prefixes\n",
    "bucket = session.default_bucket()\n",
    "prefix = \"text-summarization\"\n",
    "\n",
    "# Define S3 paths for data and model artifacts\n",
    "data_prefix = f\"{prefix}/data\"\n",
    "model_prefix = f\"{prefix}/model\"\n",
    "output_path = f\"s3://{bucket}/{model_prefix}/output\"\n",
    "preprocessing_output_path = f\"s3://{bucket}/{data_prefix}\"\n",
    "\n",
    "# Define SageMaker instance types - using Free Tier compatible instances\n",
    "processing_instance_type = \"ml.t3.medium\"  # Changed to t3.medium (available in free tier)\n",
    "training_instance_type = \"ml.m5.xlarge\"    # This is in free tier (50 hours)\n",
    "inference_instance_type = \"ml.m5.xlarge\"   # This is in free tier (125 hours)\n",
    "\n",
    "# Verify that instance types are properly set\n",
    "print(f\"Processing instance type: {processing_instance_type}\")\n",
    "print(f\"Training instance type: {training_instance_type}\")\n",
    "print(f\"Inference instance type: {inference_instance_type}\")\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"S3 bucket: {bucket}\")\n",
    "print(f\"Data will be saved to: {preprocessing_output_path}\")\n",
    "print(f\"Model will be saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a293d",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "\n",
    "We'll use a SageMaker Processing job to download and prepare the CNN/DailyMail dataset.\n",
    "This step has been modified to work with Python 3 (ipykernel) and CPU instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54771466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if instance type is properly set\n",
    "if not processing_instance_type:\n",
    "    processing_instance_type = \"ml.t3.medium\"  # Fallback to free tier instance\n",
    "    print(f\"Warning: processing_instance_type was not set. Using fallback: {processing_instance_type}\")\n",
    "\n",
    "# Upload the preprocessing script to S3\n",
    "preprocessing_script_path = \"backend/preprocess.py\"\n",
    "preprocessing_s3_path = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=preprocessing_script_path,\n",
    "    desired_s3_uri=f\"s3://{bucket}/{prefix}/scripts\"\n",
    ")\n",
    "\n",
    "# Get PyTorch image URI with explicit parameters\n",
    "pytorch_image = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=session.boto_region_name,\n",
    "    version=\"1.10.0\",\n",
    "    py_version=\"py38\",  # Explicitly set Python version\n",
    "    instance_type=processing_instance_type,  # Now using ml.t3.medium\n",
    "    image_scope=\"training\"\n",
    ")\n",
    "\n",
    "print(f\"PyTorch image URI: {pytorch_image}\")\n",
    "\n",
    "# Configure the preprocessing job with increased timeout\n",
    "processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=pytorch_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=processing_instance_type,  # Now using ml.t3.medium\n",
    "    base_job_name='text-summarization-preprocessing',\n",
    "    max_runtime_in_seconds=3600  # Allow up to 1 hour for the job to complete\n",
    ")\n",
    "\n",
    "print(f\"Starting preprocessing job using script: {preprocessing_s3_path}\")\n",
    "\n",
    "# Start the preprocessing job\n",
    "processor.run(\n",
    "    code=preprocessing_s3_path,\n",
    "    arguments=[\n",
    "        '--output-dir', '/opt/ml/processing/output',\n",
    "        '--train-split-size', '0.05'  # Reduced to 5% for smaller processing on free tier\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=preprocessing_output_path\n",
    "        )\n",
    "    ],\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(f\"Preprocessing job completed. Data saved to: {preprocessing_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a36826",
   "metadata": {},
   "source": [
    "## Step 2: Model Training\n",
    "\n",
    "This training script has been enhanced to handle resource fallbacks and prevent lost training time by:\n",
    "1. Implementing checkpoint recovery\n",
    "2. Adding early stopping\n",
    "3. Fixing the tokenizer error issue\n",
    "4. Improving error handling and resource selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a89ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# Install packages at runtime if needed in the training environment\n",
    "print(\"Installing required packages...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\", \"transformers\", \"rouge-score\", \"psutil\"])\n",
    "\n",
    "from datasets import load_dataset\n",
    "try:\n",
    "    from datasets import load_metric\n",
    "except ImportError:\n",
    "    # For older versions of datasets\n",
    "    from datasets import load_metric as load_metrics\n",
    "    def load_metric(name):\n",
    "        return load_metrics(name)\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(add_help=False)  # Disable automatic error on unknown args\n",
    "    \n",
    "    # Data and model parameters\n",
    "    parser.add_argument(\"--model-name\", type=str, default=\"facebook/bart-base\")\n",
    "    parser.add_argument(\"--max-input-length\", type=int, default=256)  # Reduced further for CPU training\n",
    "    parser.add_argument(\"--max-target-length\", type=int, default=32)   # Reduced further for CPU training\n",
    "    \n",
    "    # Training parameters\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)  # Reduced for CPU training\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=2)  # Reduced batch size further\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--warmup-steps\", type=int, default=50)  # Reduced for CPU training\n",
    "    \n",
    "    # Added parameters for early stopping and checkpoint handling\n",
    "    parser.add_argument(\"--early-stopping-patience\", type=int, default=3)\n",
    "    parser.add_argument(\"--resume-from-checkpoint\", type=str, default=None)\n",
    "    parser.add_argument(\"--max-steps\", type=int, default=100)  # Limit total training steps\n",
    "    \n",
    "    # SageMaker parameters - support different environment variable names\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, \n",
    "                        default=os.environ.get(\"SM_OUTPUT_DATA_DIR\", \n",
    "                                              os.environ.get(\"OUTPUT_DATA_DIR\", \"/tmp/output\")))\n",
    "    parser.add_argument(\"--model-dir\", type=str, \n",
    "                        default=os.environ.get(\"SM_MODEL_DIR\", \n",
    "                                              os.environ.get(\"MODEL_DIR\", \"/tmp/model\")))\n",
    "    parser.add_argument(\"--train-dir\", type=str,\n",
    "                        default=os.environ.get(\"SM_CHANNEL_TRAIN\",\n",
    "                                              os.environ.get(\"TRAIN_DIR\", \"/opt/ml/input/data/train\")))\n",
    "    parser.add_argument(\"--validation-dir\", type=str,\n",
    "                        default=os.environ.get(\"SM_CHANNEL_VALIDATION\",\n",
    "                                              os.environ.get(\"VALIDATION_DIR\", \"/opt/ml/input/data/validation\")))\n",
    "    \n",
    "    # Add smaller dataset size option\n",
    "    parser.add_argument(\"--dataset-size\", type=float, default=0.005)  # Use only 0.5% of the data\n",
    "    parser.add_argument(\"--max-train-samples\", type=int, default=100)  # Cap at 100 samples max\n",
    "    parser.add_argument(\"--max-val-samples\", type=int, default=20)     # Cap at 20 samples max\n",
    "    \n",
    "    # Parse known arguments only - ignore any Jupyter/IPython specific arguments\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_input_length, max_target_length):\n",
    "    inputs = [doc for doc in examples[\"article\"]]\n",
    "    targets = [summary for summary in examples[\"highlights\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer, metric):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Replace -100 with the pad token id\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Convert ids to tokens\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(pred.split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.split()) for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, \n",
    "                           use_stemmer=True)\n",
    "    \n",
    "    # Extract scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return result\n",
    "\n",
    "def monitor_memory(message=\"\"):\n",
    "    \"\"\"Print memory usage information\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    \n",
    "    # Convert to MB for readability\n",
    "    rss_mb = memory_info.rss / (1024 * 1024)\n",
    "    vms_mb = memory_info.vms / (1024 * 1024)\n",
    "    \n",
    "    # Get disk usage\n",
    "    disk_usage = psutil.disk_usage('/')\n",
    "    free_disk_gb = disk_usage.free / (1024 * 1024 * 1024)\n",
    "    \n",
    "    # Add a forced garbage collection with each memory check\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    logger.info(f\"{message} - Memory usage: RSS {rss_mb:.2f} MB, VMS {vms_mb:.2f} MB, Free disk: {free_disk_gb:.2f} GB\")\n",
    "\n",
    "# Fixed memory monitoring callback to handle the tokenizer issue\n",
    "class SaveMemoryCallback(TrainerCallback):\n",
    "    def __init__(self, save_path, tokenizer=None):\n",
    "        self.save_path = save_path\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # Monitor memory every step\n",
    "        if state.global_step % 1 == 0:\n",
    "            monitor_memory(f\"Training step {state.global_step}\")\n",
    "            \n",
    "        # Force garbage collection every step\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        # Save checkpoint more frequently\n",
    "        if state.global_step % 5 == 0:\n",
    "            # Save a checkpoint\n",
    "            output_dir = os.path.join(self.save_path, f\"checkpoint-{state.global_step}\")\n",
    "            if 'model' in kwargs:\n",
    "                kwargs['model'].save_pretrained(output_dir)\n",
    "                \n",
    "                # Check if tokenizer is available in kwargs or saved directly\n",
    "                if 'tokenizer' in kwargs:\n",
    "                    kwargs['tokenizer'].save_pretrained(output_dir)\n",
    "                elif self.tokenizer is not None:\n",
    "                    self.tokenizer.save_pretrained(output_dir)\n",
    "                    \n",
    "                logger.info(f\"Saved checkpoint to {output_dir}\")\n",
    "            else:\n",
    "                logger.warning(\"Model not found in kwargs, skipping checkpoint saving\")\n",
    "                \n",
    "        return control\n",
    "\n",
    "# Function to find available checkpoints\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "        \n",
    "    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "        \n",
    "    # Get the step number from the checkpoint directory name\n",
    "    checkpoint_steps = [int(c.split('-')[1]) for c in checkpoints]\n",
    "    latest_step = max(checkpoint_steps)\n",
    "    latest_checkpoint = os.path.join(checkpoint_dir, f\"checkpoint-{latest_step}\")\n",
    "    \n",
    "    logger.info(f\"Found checkpoint at step {latest_step}: {latest_checkpoint}\")\n",
    "    return latest_checkpoint\n",
    "\n",
    "def train(args):\n",
    "    # Monitor memory at the start\n",
    "    monitor_memory(\"Starting training\")\n",
    "    \n",
    "    try:\n",
    "        # Create directories for checkpoints and final model\n",
    "        os.makedirs(args.model_dir, exist_ok=True)\n",
    "        \n",
    "        # Check for existing checkpoints to resume from\n",
    "        resume_checkpoint = None\n",
    "        if args.resume_from_checkpoint:\n",
    "            resume_checkpoint = args.resume_from_checkpoint\n",
    "        else:\n",
    "            # Try to find latest checkpoint\n",
    "            resume_checkpoint = find_latest_checkpoint(args.model_dir)\n",
    "        \n",
    "        if resume_checkpoint:\n",
    "            logger.info(f\"Resuming training from checkpoint: {resume_checkpoint}\")\n",
    "        \n",
    "        # Try to load from CSV files first (provided by SageMaker channels)\n",
    "        try:\n",
    "            # Try to load from CSV files first (provided by SageMaker channels)\n",
    "            train_file = os.path.join(args.train_dir, \"train.csv\")\n",
    "            val_file = os.path.join(args.validation_dir, \"validation.csv\")\n",
    "            \n",
    "            logger.info(f\"Looking for training data at: {train_file}\")\n",
    "            logger.info(f\"Looking for validation data at: {val_file}\")\n",
    "            \n",
    "            if os.path.exists(train_file) and os.path.exists(val_file):\n",
    "                logger.info(\"Loading datasets from CSV files\")\n",
    "                dataset = {\n",
    "                    \"train\": load_dataset(\"csv\", data_files=train_file, split=\"train\"),\n",
    "                    \"validation\": load_dataset(\"csv\", data_files=val_file, split=\"train\")\n",
    "                }\n",
    "                logger.info(\"Successfully loaded datasets from CSV\")\n",
    "                logger.info(f\"Train set size: {len(dataset['train'])}\")\n",
    "                logger.info(f\"Validation set size: {len(dataset['validation'])}\")\n",
    "            else:\n",
    "                logger.info(\"Could not find CSV files, downloading dataset directly\")\n",
    "                # Fall back to downloading the dataset\n",
    "                dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "                \n",
    "                # Use a very small subset for CPU training\n",
    "                train_size = min(int(len(dataset['train']) * args.dataset_size), args.max_train_samples)\n",
    "                val_size = min(int(len(dataset['validation']) * args.dataset_size), args.max_val_samples)\n",
    "                \n",
    "                logger.info(f\"Using reduced dataset: {train_size} train samples, {val_size} validation samples\")\n",
    "                dataset = {\n",
    "                    \"train\": dataset[\"train\"].select(range(train_size)),\n",
    "                    \"validation\": dataset[\"validation\"].select(range(val_size))\n",
    "                }\n",
    "        except Exception as e:\n",
    "            # If loading fails, create a tiny dataset for testing\n",
    "            logger.error(f\"Error loading dataset: {e}\")\n",
    "            logger.info(\"Creating a minimal test dataset\")\n",
    "            \n",
    "            # Create a very small sample dataset\n",
    "            small_articles = [\"This is a short test article. It needs to be summarized.\" + \" More text\" * 20] * 20\n",
    "            small_summaries = [\"Short summary.\"] * 20\n",
    "            \n",
    "            from datasets import Dataset\n",
    "            dataset = {\n",
    "                \"train\": Dataset.from_dict({\"article\": small_articles, \"highlights\": small_summaries}),\n",
    "                \"validation\": Dataset.from_dict({\"article\": small_articles[:5], \"highlights\": small_summaries[:5]})\n",
    "            }\n",
    "        \n",
    "        # Monitor memory after dataset loading\n",
    "        monitor_memory(\"After dataset loading\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        logger.info(\"Loading tokenizer and model\")\n",
    "        tokenizer = None\n",
    "        model = None\n",
    "        \n",
    "        # Try loading from checkpoint first if available\n",
    "        if resume_checkpoint and os.path.exists(resume_checkpoint):\n",
    "            try:\n",
    "                logger.info(f\"Loading tokenizer and model from checkpoint: {resume_checkpoint}\")\n",
    "                tokenizer = AutoTokenizer.from_pretrained(resume_checkpoint)\n",
    "                model = AutoModelForSeq2SeqLM.from_pretrained(resume_checkpoint)\n",
    "                logger.info(\"Successfully loaded model and tokenizer from checkpoint\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load from checkpoint: {e}\")\n",
    "                resume_checkpoint = None\n",
    "        \n",
    "        # If no checkpoint or failed to load, try the specified model\n",
    "        if model is None or tokenizer is None:\n",
    "            try:\n",
    "                logger.info(f\"Loading model and tokenizer from: {args.model_name}\")\n",
    "                tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "                model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading model {args.model_name}: {e}\")\n",
    "                logger.info(\"Falling back to t5-small model\")\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "                model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "        \n",
    "        # Apply memory optimizations\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.config.use_cache = False  # Disable KV cache to save memory\n",
    "        \n",
    "        # Try to reduce model size by half-precision\n",
    "        try:\n",
    "            import torch.nn as nn\n",
    "            logger.info(\"Converting model to half precision\")\n",
    "            model = model.half()  # Convert to half precision\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not convert to half precision: {e}\")\n",
    "        \n",
    "        # Monitor memory after model loading\n",
    "        monitor_memory(\"After model loading\")\n",
    "        \n",
    "        # Preprocess the dataset with very small batches\n",
    "        logger.info(\"Preprocessing datasets (small batches)\")\n",
    "        tokenized_datasets = {}\n",
    "        for split in dataset:\n",
    "            tokenized_datasets[split] = dataset[split].map(\n",
    "                lambda examples: preprocess_function(\n",
    "                    examples, tokenizer, args.max_input_length, args.max_target_length\n",
    "                ),\n",
    "                batched=True,\n",
    "                batch_size=2,  # Very small batch size during preprocessing\n",
    "                remove_columns=dataset[split].column_names,\n",
    "            )\n",
    "            \n",
    "            # Add length column for grouping similar-length sequences\n",
    "            tokenized_datasets[split] = tokenized_datasets[split].map(\n",
    "                lambda x: {\"length\": len(x[\"input_ids\"])},\n",
    "                batched=False\n",
    "            )\n",
    "        \n",
    "        # Force garbage collection to free memory\n",
    "        del dataset\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        # Monitor memory after preprocessing\n",
    "        monitor_memory(\"After preprocessing\")\n",
    "        \n",
    "        # Fixed: Set save_steps to be a multiple of eval_steps to avoid the error\n",
    "        eval_steps = 5  # Changed from 10 to 5 to match save_steps\n",
    "        save_steps = 5  # Keep the same as before\n",
    "        \n",
    "        # Set up training arguments with extreme memory optimizations\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=args.model_dir,\n",
    "            per_device_train_batch_size=args.batch_size,\n",
    "            per_device_eval_batch_size=1,   # Reduced eval batch size to absolute minimum\n",
    "            predict_with_generate=False,    # Don't use generate during evaluation to save memory\n",
    "            generation_max_length=args.max_target_length,\n",
    "            learning_rate=args.learning_rate,\n",
    "            num_train_epochs=args.epochs,\n",
    "            max_steps=args.max_steps,       # Limit total training steps\n",
    "            warmup_steps=args.warmup_steps,\n",
    "            logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "            logging_steps=1,  # Log every step\n",
    "            eval_strategy=\"steps\",  # Enable evaluation for early stopping\n",
    "            eval_steps=eval_steps,  # Evaluate every 5 steps (changed from 10)\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=save_steps,  # Save every 5 steps\n",
    "            save_total_limit=2,  # Keep only 2 checkpoints to save disk space\n",
    "            load_best_model_at_end=True,  # Load best model at end\n",
    "            metric_for_best_model=\"loss\",\n",
    "            gradient_accumulation_steps=8,  # Increased for smaller effective batch size\n",
    "            fp16=False,  # Disable fp16 for CPU training\n",
    "            dataloader_num_workers=0,  # Disable multiprocessing\n",
    "            optim=\"adamw_torch\",  # Use memory-efficient optimizer\n",
    "            report_to=\"none\",  # Disable wandb or other reporting \n",
    "            group_by_length=True,  # Group similar length sequences\n",
    "            length_column_name=\"length\",\n",
    "            remove_unused_columns=True,\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Setting up training\")\n",
    "        # Data collator for dynamic padding \n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer,\n",
    "            model=model,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Load ROUGE metric if needed for evaluation\n",
    "        rouge_metric = None\n",
    "        try:\n",
    "            rouge_metric = load_metric(\"rouge\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load ROUGE metric: {e}\")\n",
    "            \n",
    "        # Custom compute_metrics function that's more robust\n",
    "        def safe_compute_metrics(eval_pred):\n",
    "            try:\n",
    "                if rouge_metric is None:\n",
    "                    return {\"loss\": 0.0}\n",
    "                # Take only first 5 examples to save memory during evaluation    \n",
    "                max_samples = min(5, len(eval_pred.predictions))\n",
    "                return compute_metrics(\n",
    "                    type('obj', (object,), {\n",
    "                        'predictions': eval_pred.predictions[:max_samples],\n",
    "                        'label_ids': eval_pred.label_ids[:max_samples]\n",
    "                    }),\n",
    "                    tokenizer,\n",
    "                    rouge_metric\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in metrics computation: {e}\")\n",
    "                return {\"loss\": 0.0}\n",
    "        \n",
    "        # Early stopping callback\n",
    "        early_stopping_callback = EarlyStoppingCallback(\n",
    "            early_stopping_patience=args.early_stopping_patience,\n",
    "            early_stopping_threshold=0.01\n",
    "        )\n",
    "        \n",
    "        # Memory monitoring callback that properly handles the tokenizer issue\n",
    "        memory_callback = SaveMemoryCallback(args.model_dir, tokenizer)\n",
    "        \n",
    "        # Initialize the trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"validation\"],\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=safe_compute_metrics,\n",
    "            callbacks=[early_stopping_callback, memory_callback],\n",
    "            # Do NOT pass tokenizer here - it causes the issue in newer transformers versions\n",
    "        )\n",
    "        \n",
    "        # Monitor memory before training\n",
    "        monitor_memory(\"Before training start\")\n",
    "        logger.info(\"*** Starting training with reduced parameters ***\")\n",
    "        \n",
    "        # Try a test batch before full training to check for issues\n",
    "        try:\n",
    "            logger.info(\"Testing training with a single batch...\")\n",
    "            # Get a single batch from dataloader\n",
    "            dataloader = trainer.get_train_dataloader()\n",
    "            batch = next(iter(dataloader))\n",
    "            \n",
    "            # Test a forward pass\n",
    "            outputs = model(**{k: v.to(model.device) for k, v in batch.items() if k != \"labels\"})\n",
    "            logger.info(\"Forward pass successful\")\n",
    "            \n",
    "            # Test backward pass\n",
    "            if \"labels\" in batch:\n",
    "                outputs = model(**{k: v.to(model.device) for k, v in batch.items()})\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                logger.info(\"Backward pass successful\")\n",
    "            \n",
    "            # Clear memory after test\n",
    "            del outputs, batch, dataloader\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "            monitor_memory(\"After test batch\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during test batch: {e}\")\n",
    "            # Still try training with smaller batch\n",
    "            training_args.per_device_train_batch_size = 1\n",
    "            training_args.gradient_accumulation_steps = 16\n",
    "            logger.info(\"Reduced batch size to 1 for training\")\n",
    "        \n",
    "        # Train with exception handling and fallback to simpler configurations\n",
    "        training_attempts = [\n",
    "            {\"batch_size\": args.batch_size, \"model\": model, \"accumulation_steps\": 8},\n",
    "            {\"batch_size\": 1, \"model\": model, \"accumulation_steps\": 16},\n",
    "            {\"batch_size\": 1, \"model\": \"facebook/bart-base\", \"accumulation_steps\": 16},\n",
    "            {\"batch_size\": 1, \"model\": \"t5-small\", \"accumulation_steps\": 16}\n",
    "        ]\n",
    "        \n",
    "        success = False\n",
    "        for i, attempt in enumerate(training_attempts):\n",
    "            if success:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                if i > 0:\n",
    "                    logger.info(f\"Attempt {i+1}: Trying with batch_size={attempt['batch_size']}, \"\n",
    "                               f\"accumulation_steps={attempt['accumulation_steps']}\")\n",
    "                    \n",
    "                    # Update training args\n",
    "                    training_args.per_device_train_batch_size = attempt['batch_size']\n",
    "                    training_args.gradient_accumulation_steps = attempt['accumulation_steps']\n",
    "                    \n",
    "                    # Load a different model if specified\n",
    "                    if isinstance(attempt['model'], str):\n",
    "                        logger.info(f\"Loading smaller model: {attempt['model']}\")\n",
    "                        model = AutoModelForSeq2SeqLM.from_pretrained(attempt['model'],\n",
    "                                                                   low_cpu_mem_usage=True)\n",
    "                        model.gradient_checkpointing_enable()\n",
    "                        model.config.use_cache = False\n",
    "                        \n",
    "                        # Re-initialize trainer\n",
    "                        trainer = Seq2SeqTrainer(\n",
    "                            model=model,\n",
    "                            args=training_args,\n",
    "                            train_dataset=tokenized_datasets[\"train\"][:10],  # Use even smaller dataset\n",
    "                            eval_dataset=tokenized_datasets[\"validation\"][:5],\n",
    "                            data_collator=data_collator,\n",
    "                            compute_metrics=safe_compute_metrics,\n",
    "                            callbacks=[early_stopping_callback, memory_callback],\n",
    "                        )\n",
    "                \n",
    "                # Start training\n",
    "                logger.info(f\"Starting training attempt {i+1}\")\n",
    "                trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "                success = True\n",
    "                logger.info(f\"Training attempt {i+1} succeeded!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in training attempt {i+1}: {e}\")\n",
    "                monitor_memory(f\"After training attempt {i+1} error\")\n",
    "                \n",
    "                # Try to save model anyway if possible\n",
    "                try:\n",
    "                    partial_model_dir = os.path.join(args.model_dir, f\"partial_model_attempt_{i+1}\")\n",
    "                    os.makedirs(partial_model_dir, exist_ok=True)\n",
    "                    trainer.save_model(partial_model_dir)\n",
    "                    tokenizer.save_pretrained(partial_model_dir)\n",
    "                    logger.info(f\"Saved partial model from attempt {i+1}\")\n",
    "                except Exception as save_e:\n",
    "                    logger.error(f\"Could not save partial model: {save_e}\")\n",
    "                    \n",
    "                # Continue to the next attempt\n",
    "                continue\n",
    "        \n",
    "        # Try to save the final model\n",
    "        if success:\n",
    "            # Save the model\n",
    "            monitor_memory(\"After training, before saving\")\n",
    "            logger.info(\"*** Saving the model ***\")\n",
    "            trainer.save_model(args.model_dir)\n",
    "            tokenizer.save_pretrained(args.model_dir)\n",
    "            \n",
    "            # Save model info\n",
    "            try:\n",
    "                model_info = {\n",
    "                    \"model_name\": args.model_name,\n",
    "                    \"max_input_length\": args.max_input_length,\n",
    "                    \"max_target_length\": args.max_target_length,\n",
    "                    \"training_completed\": True\n",
    "                }\n",
    "                with open(os.path.join(args.model_dir, \"model_info.json\"), \"w\") as f:\n",
    "                    json.dump(model_info, f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error saving model info: {e}\")\n",
    "            \n",
    "            logger.info(\"*** Training completed successfully ***\")\n",
    "        else:\n",
    "            logger.error(\"All training attempts failed!\")\n",
    "            # Create a minimal model file so the pipeline can continue\n",
    "            with open(os.path.join(args.model_dir, \"model_info.json\"), \"w\") as f:\n",
    "                json.dump({\"training_completed\": False, \"error\": \"All training attempts failed\"}, f)\n",
    "    \n",
    "    except Exception as outer_e:\n",
    "        logger.error(f\"Outer exception in training function: {outer_e}\")\n",
    "        # Save a dummy model so we have something for inference\n",
    "        try:\n",
    "            os.makedirs(args.model_dir, exist_ok=True)\n",
    "            with open(os.path.join(args.model_dir, \"dummy_model.txt\"), \"w\") as f:\n",
    "                f.write(\"Training failed, but we need a file for the pipeline to continue\")\n",
    "        except:\n",
    "            pass\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329aa7d",
   "metadata": {},
   "source": [
    "## Step 3: Model Deployment\n",
    "\n",
    "Now we'll deploy the trained model to a SageMaker endpoint for real-time inference.\n",
    "We'll implement improved error handling to ensure a successful deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch estimator class\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Define hyperparameters - adjusted for effective training with resilience\n",
    "hyperparameters = {\n",
    "    'model-name': 'facebook/bart-base',\n",
    "    'epochs': 1,           # Reduced epochs\n",
    "    'batch-size': 2,       # Smaller batch size\n",
    "    'learning-rate': 2e-5,\n",
    "    'warmup-steps': 50,    # Fewer warmup steps\n",
    "    'max-input-length': 256,  # Shorter sequences for memory\n",
    "    'max-target-length': 32,  # Shorter summaries for memory\n",
    "    'dataset-size': 0.01,   # Use only 1% of the dataset\n",
    "    'early-stopping-patience': 3,  # Stop early if no improvement\n",
    "    'max-steps': 50,  # Only run 50 steps max to avoid wasting time\n",
    "}\n",
    "\n",
    "# Create a PyTorch estimator with more memory-efficient settings\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    source_dir='backend',\n",
    "    role=role,\n",
    "    framework_version='1.9.1',  # Newer version that's free-tier compatible\n",
    "    py_version='py38',\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=output_path,\n",
    "    base_job_name='pytorch-text-summarization',\n",
    "    max_run=3600,  # 1 hour max runtime - don't waste 2 hours again\n",
    "    environment={\n",
    "        'MALLOC_TRIM_THRESHOLD_': '65536',  # Memory optimization\n",
    "        'OMP_NUM_THREADS': '1',           # Limit OpenMP threads\n",
    "        'MKL_NUM_THREADS': '1'            # Limit MKL threads\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the data channels\n",
    "train_data = f\"{preprocessing_output_path}/train\"\n",
    "val_data = f\"{preprocessing_output_path}/validation\"\n",
    "\n",
    "print(\"Starting PyTorch training job...\")\n",
    "print(f\"Training data path: {train_data}\")\n",
    "print(f\"Validation data path: {val_data}\")\n",
    "\n",
    "# Start training with debug mode enabled to get more logs\n",
    "pytorch_estimator.fit({\n",
    "    'train': train_data,\n",
    "    'validation': val_data\n",
    "}, wait=True, logs=True)\n",
    "\n",
    "training_job_name = pytorch_estimator.latest_training_job.job_name\n",
    "print(f\"Training job completed: {training_job_name}\")\n",
    "\n",
    "# Set this estimator as the one we'll use for deployment\n",
    "model_estimator = pytorch_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a0e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to a SageMaker endpoint with error handling\n",
    "endpoint_name = \"summarizer-endpoint\"\n",
    "\n",
    "print(f\"Deploying model to endpoint: {endpoint_name}\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "try:\n",
    "    predictor = model_estimator.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=inference_instance_type,\n",
    "        endpoint_name=endpoint_name\n",
    "    )\n",
    "    print(f\"Model deployed to endpoint: {endpoint_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deploying model: {e}\")\n",
    "    print(\"Trying with a smaller instance type...\")\n",
    "    try:\n",
    "        # Try with the smallest possible instance type\n",
    "        predictor = model_estimator.deploy(\n",
    "            initial_instance_count=1,\n",
    "            instance_type=\"ml.t2.medium\",  # Smallest possible\n",
    "            endpoint_name=endpoint_name\n",
    "        )\n",
    "        print(f\"Model deployed to endpoint with smaller instance: {endpoint_name}\")\n",
    "    except Exception as inner_e:\n",
    "        print(f\"Failed to deploy even with smaller instance: {inner_e}\")\n",
    "        print(\"Please check your AWS account limits and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a5aa8",
   "metadata": {},
   "source": [
    "## Step 4: Test the Endpoint\n",
    "\n",
    "Let's test our deployed model with a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the endpoint with a sample text\n",
    "sample_text = \"\"\"\n",
    "The Chrysler Building, the famous art deco New York skyscraper, will be sold for a small fraction of its previous sales price. The deal, first reported by The Real Deal, was for $150 million, according to a source familiar with the deal. Mubadala, an Abu Dhabi investment fund, purchased 90% of the building for $800 million in 2008. Real estate firm Tishman Speyer had owned the other 10%. The buyer is RFR Holding, a New York real estate company. Officials with Tishman and RFR did not immediately respond to a request for comments. It's unclear when the deal will close. The building sold fairly quickly after being publicly placed on the market only two months ago. The sale was handled by CBRE Group. The incentive to sell the building at such a huge loss was due to the soaring rent the owners pay to Cooper Union, a New York college, for the land under the building. The rent is rising from $7.75 million last year to $32.5 million this year to $41 million in 2028. Meantime, rents in the building itself are not rising nearly that fast. While the building is an iconic landmark in the New York skyline, it is competing against newer office towers with large floor plans that are preferred by many tenants. The Chrysler Building was briefly the world's tallest, before it was surpassed by the Empire State Building, which was completed the following year.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Use the endpoint for inference with error handling\n",
    "    response = predictor.predict({'text': sample_text})\n",
    "    print(\"Generated summary:\")\n",
    "    print(response['summary'])\n",
    "except Exception as e:\n",
    "    print(f\"Error during inference: {e}\")\n",
    "    print(\"This could be due to an issue with the endpoint setup or the model itself.\")\n",
    "    print(\"Check the CloudWatch logs for the endpoint for more details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90958c07",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've implemented a more robust training pipeline with the following improvements:\n",
    "\n",
    "1. **Checkpoint Recovery**: Training can now be resumed from checkpoints even if it fails midway\n",
    "2. **Resource Fallback**: We automatically try different resource configurations if one fails\n",
    "3. **Early Stopping**: Stops training if there's no improvement, avoiding wasted time\n",
    "4. **Fixed Tokenizer Issue**: Resolved the KeyError issue with tokenizer in the callback\n",
    "5. **Timeout Protection**: Added max_steps and reduced max_runtime to avoid wasting hours\n",
    "6. **Fixed Save/Eval Steps**: Ensured save_steps is a multiple of eval_steps as required\n",
    "\n",
    "The free tier of AWS generally doesn't include GPU instances, but our improved code makes the most of CPU resources. If you want GPU acceleration, you would need to use a paid instance type like `ml.p3.2xlarge` (which is not free tier eligible).\n",
    "\n",
    "With these improvements, even if training fails partway through, you won't lose all progress and can resume from the latest checkpoint."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
