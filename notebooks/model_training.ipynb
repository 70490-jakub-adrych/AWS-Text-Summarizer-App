{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f15c158",
   "metadata": {},
   "source": [
    "# Text Summarization Model Training\n",
    "\n",
    "This notebook demonstrates how to fine-tune a transformer model (BART) on the CNN/DailyMail dataset using SageMaker for text summarization.\n",
    "\n",
    "**Note**: This notebook is configured to run with the **Python 3 (ipykernel)** kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd60ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries - expanded to include all necessary dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install boto3 sagemaker\n",
    "!pip install transformers datasets\n",
    "!pip install torch torchvision\n",
    "!pip install pandas numpy scikit-learn\n",
    "!pip install rouge-score nltk\n",
    "\n",
    "# Verify CUDA is not available (CPU training)\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count() if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up working directory for backend files\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Create backend directory if it doesn't exist\n",
    "!mkdir -p backend\n",
    "\n",
    "# If backend files are not already in the notebook instance, we need to create them\n",
    "# Check if train.py exists in backend directory\n",
    "if not os.path.exists('backend/train.py'):\n",
    "    print(\"Creating backend files...\")\n",
    "    # We'll create these files later, but for now let's check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362c891a",
   "metadata": {},
   "source": [
    "## Creating Backend Files\n",
    "\n",
    "Since we're working directly in the notebook instance, we need to create the required backend files. Let's create the preprocess.py and train.py files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocess.py file\n",
    "%%writefile backend/preprocess.py\n",
    "\"\"\"\n",
    "This script downloads and prepares the CNN/DailyMail dataset for SageMaker training.\n",
    "It runs as a SageMaker Processing job to prepare the data.\n",
    "\"\"\"\n",
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output-dir\", type=str, required=True)\n",
    "    parser.add_argument(\"--train-split-size\", type=float, default=0.1)  # Reduced to 10% for faster processing\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(os.path.join(args.output_dir, \"train\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(args.output_dir, \"validation\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(args.output_dir, \"test\"), exist_ok=True)\n",
    "    \n",
    "    logger.info(\"Downloading CNN/DailyMail dataset...\")\n",
    "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "    \n",
    "    # We'll use a smaller subset for faster processing\n",
    "    train_size = int(len(dataset['train']) * args.train_split_size)\n",
    "    val_size = int(len(dataset['validation']) * args.train_split_size)\n",
    "    test_size = int(len(dataset['test']) * args.train_split_size)\n",
    "    \n",
    "    logger.info(f\"Processing training split (using {train_size} samples)...\")\n",
    "    train_df = pd.DataFrame({\n",
    "        \"article\": dataset[\"train\"][\"article\"][:train_size],\n",
    "        \"highlights\": dataset[\"train\"][\"highlights\"][:train_size]\n",
    "    })\n",
    "    train_df.to_csv(os.path.join(args.output_dir, \"train\", \"train.csv\"), index=False)\n",
    "    \n",
    "    logger.info(f\"Processing validation split (using {val_size} samples)...\")\n",
    "    val_df = pd.DataFrame({\n",
    "        \"article\": dataset[\"validation\"][\"article\"][:val_size],\n",
    "        \"highlights\": dataset[\"validation\"][\"highlights\"][:val_size]\n",
    "    })\n",
    "    val_df.to_csv(os.path.join(args.output_dir, \"validation\", \"validation.csv\"), index=False)\n",
    "    \n",
    "    logger.info(f\"Processing test split (using {test_size} samples)...\")\n",
    "    test_df = pd.DataFrame({\n",
    "        \"article\": dataset[\"test\"][\"article\"][:test_size],\n",
    "        \"highlights\": dataset[\"test\"][\"highlights\"][:test_size]\n",
    "    })\n",
    "    test_df.to_csv(os.path.join(args.output_dir, \"test\", \"test.csv\"), index=False)\n",
    "    \n",
    "    logger.info(\"Data processing completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32373a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train.py file\n",
    "%%writefile backend/train.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Data and model parameters\n",
    "    parser.add_argument(\"--model-name\", type=str, default=\"facebook/bart-base\")\n",
    "    parser.add_argument(\"--max-input-length\", type=int, default=512)  # Reduced for CPU training\n",
    "    parser.add_argument(\"--max-target-length\", type=int, default=64)   # Reduced for CPU training\n",
    "    \n",
    "    # Training parameters\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)  # Reduced for CPU training\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=4)  # Reduced for CPU training\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--warmup-steps\", type=int, default=100)  # Reduced for CPU training\n",
    "    \n",
    "    # SageMaker parameters\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\", \"/tmp/output\"))\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\", \"/tmp/model\"))\n",
    "    parser.add_argument(\"--n-gpus\", type=int, default=0)  # Default to CPU training\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_input_length, max_target_length):\n",
    "    inputs = [doc for doc in examples[\"article\"]]\n",
    "    targets = [summary for summary in examples[\"highlights\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer, metric):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Replace -100 with the pad token id\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Convert ids to tokens\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(pred.split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.split()) for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, \n",
    "                          use_stemmer=True)\n",
    "    \n",
    "    # Extract scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return result\n",
    "\n",
    "def train(args):\n",
    "    # Load the CNN/DailyMail dataset\n",
    "    try:\n",
    "        # Try to load from CSV first (for SageMaker Processing outputs)\n",
    "        dataset = {\n",
    "            \"train\": load_dataset(\"csv\", data_files=\"/opt/ml/input/data/train/train.csv\", split=\"train\"),\n",
    "            \"validation\": load_dataset(\"csv\", data_files=\"/opt/ml/input/data/validation/validation.csv\", split=\"train\")\n",
    "        }\n",
    "        logger.info(\"Loaded dataset from CSV files\")\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Could not load from CSV, downloading directly: {e}\")\n",
    "        # Fall back to downloading the dataset\n",
    "        full_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "        # Use a small subset for CPU training (10%)\n",
    "        train_size = int(len(full_dataset['train']) * 0.1)\n",
    "        val_size = int(len(full_dataset['validation']) * 0.1)\n",
    "        dataset = {\n",
    "            \"train\": full_dataset[\"train\"].select(range(train_size)),\n",
    "            \"validation\": full_dataset[\"validation\"].select(range(val_size))\n",
    "        }\n",
    "    \n",
    "    # Load the pre-trained model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name)\n",
    "    \n",
    "    # Preprocess the dataset\n",
    "    tokenized_datasets = {}\n",
    "    for split in dataset:\n",
    "        tokenized_datasets[split] = dataset[split].map(\n",
    "            lambda examples: preprocess_function(\n",
    "                examples, tokenizer, args.max_input_length, args.max_target_length\n",
    "            ),\n",
    "            batched=True,\n",
    "            remove_columns=dataset[split].column_names,\n",
    "        )\n",
    "    \n",
    "    # Data collator for dynamic padding\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Load ROUGE metric\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    \n",
    "    # Set up training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=args.model_dir,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=args.max_target_length,\n",
    "        learning_rate=args.learning_rate,\n",
    "        num_train_epochs=args.epochs,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "        logging_steps=10,  # More frequent logging for small dataset\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rouge1\",\n",
    "        gradient_accumulation_steps=1,  # Reduced for CPU training\n",
    "        # Disable fp16 for CPU training\n",
    "        fp16=False\n",
    "    )\n",
    "    \n",
    "    # Initialize the trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer, rouge_metric),\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    logger.info(\"*** Training the model ***\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    logger.info(\"*** Saving the model ***\")\n",
    "    trainer.save_model(args.model_dir)\n",
    "    tokenizer.save_pretrained(args.model_dir)\n",
    "    \n",
    "    logger.info(\"*** Training completed ***\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa661e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# Initialize SageMaker session\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define S3 bucket and prefixes\n",
    "bucket = session.default_bucket()\n",
    "prefix = \"text-summarization\"\n",
    "\n",
    "# Define S3 paths for data and model artifacts\n",
    "data_prefix = f\"{prefix}/data\"\n",
    "model_prefix = f\"{prefix}/model\"\n",
    "output_path = f\"s3://{bucket}/{model_prefix}/output\"\n",
    "preprocessing_output_path = f\"s3://{bucket}/{data_prefix}\"\n",
    "\n",
    "# Define SageMaker instance types - using Free Tier compatible instances\n",
    "processing_instance_type = \"ml.t3.medium\"  # Changed to t3.medium (available in free tier)\n",
    "training_instance_type = \"ml.m5.xlarge\"    # This is in free tier (50 hours)\n",
    "inference_instance_type = \"ml.m5.xlarge\"   # This is in free tier (125 hours)\n",
    "\n",
    "# Verify that instance types are properly set\n",
    "print(f\"Processing instance type: {processing_instance_type}\")\n",
    "print(f\"Training instance type: {training_instance_type}\")\n",
    "print(f\"Inference instance type: {inference_instance_type}\")\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"S3 bucket: {bucket}\")\n",
    "print(f\"Data will be saved to: {preprocessing_output_path}\")\n",
    "print(f\"Model will be saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a293d",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "\n",
    "We'll use a SageMaker Processing job to download and prepare the CNN/DailyMail dataset.\n",
    "This step has been modified to work with Python 3 (ipykernel) and CPU instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9349b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update preprocess.py to install dependencies at runtime\n",
    "%%writefile backend/preprocess.py\n",
    "\"\"\"\n",
    "This script downloads and prepares the CNN/DailyMail dataset for SageMaker training.\n",
    "It runs as a SageMaker Processing job to prepare the data.\n",
    "\"\"\"\n",
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install required packages at runtime\n",
    "print(\"Installing required packages...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\", \"pandas\", \"transformers\"])\n",
    "\n",
    "# Now we can import the required modules\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output-dir\", type=str, required=True)\n",
    "    parser.add_argument(\"--train-split-size\", type=float, default=0.05)  # Reduced to 5% for faster processing\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(os.path.join(args.output_dir, \"train\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(args.output_dir, \"validation\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(args.output_dir, \"test\"), exist_ok=True)\n",
    "    \n",
    "    logger.info(\"Downloading CNN/DailyMail dataset...\")\n",
    "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "    \n",
    "    # We'll use a smaller subset for faster processing\n",
    "    train_size = int(len(dataset['train']) * args.train_split_size)\n",
    "    val_size = int(len(dataset['validation']) * args.train_split_size)\n",
    "    test_size = int(len(dataset['test']) * args.train_split_size)\n",
    "    \n",
    "    logger.info(f\"Processing training split (using {train_size} samples)...\")\n",
    "    train_df = pd.DataFrame({\n",
    "        \"article\": dataset[\"train\"][\"article\"][:train_size],\n",
    "        \"highlights\": dataset[\"train\"][\"highlights\"][:train_size]\n",
    "    })\n",
    "    train_df.to_csv(os.path.join(args.output_dir, \"train\", \"train.csv\"), index=False)\n",
    "    \n",
    "    logger.info(f\"Processing validation split (using {val_size} samples)...\")\n",
    "    val_df = pd.DataFrame({\n",
    "        \"article\": dataset[\"validation\"][\"article\"][:val_size],\n",
    "        \"highlights\": dataset[\"validation\"][\"highlights\"][:val_size]\n",
    "    })\n",
    "    val_df.to_csv(os.path.join(args.output_dir, \"validation\", \"validation.csv\"), index=False)\n",
    "    \n",
    "    logger.info(f\"Processing test split (using {test_size} samples)...\")\n",
    "    test_df = pd.DataFrame({\n",
    "        \"article\": dataset[\"test\"][\"article\"][:test_size],\n",
    "        \"highlights\": dataset[\"test\"][\"highlights\"][:test_size]\n",
    "    })\n",
    "    test_df.to_csv(os.path.join(args.output_dir, \"test\", \"test.csv\"), index=False)\n",
    "    \n",
    "    logger.info(\"Data processing completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54771466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if instance type is properly set\n",
    "if not processing_instance_type:\n",
    "    processing_instance_type = \"ml.t3.medium\"  # Fallback to free tier instance\n",
    "    print(f\"Warning: processing_instance_type was not set. Using fallback: {processing_instance_type}\")\n",
    "\n",
    "# Upload the preprocessing script to S3\n",
    "preprocessing_script_path = \"backend/preprocess.py\"\n",
    "preprocessing_s3_path = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=preprocessing_script_path,\n",
    "    desired_s3_uri=f\"s3://{bucket}/{prefix}/scripts\"\n",
    ")\n",
    "\n",
    "# Get PyTorch image URI with explicit parameters\n",
    "pytorch_image = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=session.boto_region_name,\n",
    "    version=\"1.10.0\",\n",
    "    py_version=\"py38\",  # Explicitly set Python version\n",
    "    instance_type=processing_instance_type,  # Now using ml.t3.medium\n",
    "    image_scope=\"training\"\n",
    ")\n",
    "\n",
    "print(f\"PyTorch image URI: {pytorch_image}\")\n",
    "\n",
    "# Configure the preprocessing job with increased timeout\n",
    "processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=pytorch_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=processing_instance_type,  # Now using ml.t3.medium\n",
    "    base_job_name='text-summarization-preprocessing',\n",
    "    max_runtime_in_seconds=3600  # Allow up to 1 hour for the job to complete\n",
    ")\n",
    "\n",
    "print(f\"Starting preprocessing job using script: {preprocessing_s3_path}\")\n",
    "\n",
    "# Start the preprocessing job\n",
    "processor.run(\n",
    "    code=preprocessing_s3_path,\n",
    "    arguments=[\n",
    "        '--output-dir', '/opt/ml/processing/output',\n",
    "        '--train-split-size', '0.05'  # Reduced to 5% for smaller processing on free tier\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=preprocessing_output_path\n",
    "        )\n",
    "    ],\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(f\"Preprocessing job completed. Data saved to: {preprocessing_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a36826",
   "metadata": {},
   "source": [
    "## Step 2: Model Training\n",
    "\n",
    "We'll fine-tune a pre-trained BART model using the HuggingFace estimator in SageMaker.\n",
    "This has been modified for CPU training with smaller hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a89ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters - adjusted for CPU training\n",
    "hyperparameters = {\n",
    "    'model-name': 'facebook/bart-base',\n",
    "    'epochs': 1,           # Reduced epochs\n",
    "    'batch-size': 4,       # Smaller batch size\n",
    "    'learning-rate': 2e-5,\n",
    "    'warmup-steps': 100,   # Fewer warmup steps\n",
    "    'max-input-length': 512,  # Shorter sequences\n",
    "    'max-target-length': 64   # Shorter summaries\n",
    "}\n",
    "\n",
    "# Upload the training script to S3\n",
    "training_script_path = \"backend/train.py\"\n",
    "\n",
    "# Create an HuggingFace estimator with versions compatible with CPU training\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='backend',  # Directory containing the script\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    transformers_version='4.6',  # Changed from 4.12 to 4.6\n",
    "    pytorch_version='1.7',      # Changed from 1.9 to 1.7\n",
    "    py_version='py36',         # Changed from py38 to py36\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=output_path,\n",
    "    base_job_name='text-summarization-training'\n",
    ")\n",
    "\n",
    "# Define the data channels\n",
    "train_data = f\"{preprocessing_output_path}/train\"\n",
    "val_data = f\"{preprocessing_output_path}/validation\"\n",
    "\n",
    "print(\"Starting training job...\")\n",
    "print(f\"Training data path: {train_data}\")\n",
    "print(f\"Validation data path: {val_data}\")\n",
    "\n",
    "# Start the training job\n",
    "huggingface_estimator.fit(\n",
    "    inputs={\n",
    "        'train': train_data,\n",
    "        'validation': val_data\n",
    "    },\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "# Get the training job name\n",
    "training_job_name = huggingface_estimator.latest_training_job.job_name\n",
    "print(f\"Training job completed: {training_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e9ac7",
   "metadata": {},
   "source": [
    "## Alternative Training Approach (If HuggingFace CPU Training Fails)\n",
    "\n",
    "If the HuggingFace estimator doesn't support CPU training with the available versions,\n",
    "we can use a PyTorch estimator instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach using PyTorch estimator if HuggingFace CPU training fails\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Define hyperparameters - adjusted for CPU training\n",
    "hyperparameters = {\n",
    "    'model-name': 'facebook/bart-base',\n",
    "    'epochs': 1,           # Reduced epochs\n",
    "    'batch-size': 4,       # Smaller batch size\n",
    "    'learning-rate': 2e-5,\n",
    "    'warmup-steps': 100,   # Fewer warmup steps\n",
    "    'max-input-length': 512,  # Shorter sequences\n",
    "    'max-target-length': 64   # Shorter summaries\n",
    "}\n",
    "\n",
    "# Create a PyTorch estimator\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    source_dir='backend',\n",
    "    role=role,\n",
    "    framework_version='1.7.1',\n",
    "    py_version='py3',\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=output_path,\n",
    "    base_job_name='pytorch-text-summarization'\n",
    ")\n",
    "\n",
    "# Uncomment to use this approach if the HuggingFace estimator fails\n",
    "# print(\"Starting PyTorch training job...\")\n",
    "# pytorch_estimator.fit({\n",
    "#     'train': train_data,\n",
    "#     'validation': val_data\n",
    "# }, wait=True)\n",
    "#\n",
    "# training_job_name = pytorch_estimator.latest_training_job.job_name\n",
    "# print(f\"Training job completed: {training_job_name}\")\n",
    "# \n",
    "# # For deployment, use this estimator instead of huggingface_estimator\n",
    "# huggingface_estimator = pytorch_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329aa7d",
   "metadata": {},
   "source": [
    "## Step 3: Model Deployment\n",
    "\n",
    "Now we'll deploy the trained model to a SageMaker endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to a SageMaker endpoint\n",
    "endpoint_name = \"summarizer-endpoint\"\n",
    "\n",
    "print(f\"Deploying model to endpoint: {endpoint_name}\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "predictor = huggingface_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a5aa8",
   "metadata": {},
   "source": [
    "## Step 4: Test the Endpoint\n",
    "\n",
    "Let's test our deployed model with a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the endpoint with a sample text\n",
    "sample_text = \"\"\"\n",
    "The Chrysler Building, the famous art deco New York skyscraper, will be sold for a small fraction of its previous sales price. The deal, first reported by The Real Deal, was for $150 million, according to a source familiar with the deal. Mubadala, an Abu Dhabi investment fund, purchased 90% of the building for $800 million in 2008. Real estate firm Tishman Speyer had owned the other 10%. The buyer is RFR Holding, a New York real estate company. Officials with Tishman and RFR did not immediately respond to a request for comments. It's unclear when the deal will close. The building sold fairly quickly after being publicly placed on the market only two months ago. The sale was handled by CBRE Group. The incentive to sell the building at such a huge loss was due to the soaring rent the owners pay to Cooper Union, a New York college, for the land under the building. The rent is rising from $7.75 million last year to $32.5 million this year to $41 million in 2028. Meantime, rents in the building itself are not rising nearly that fast. While the building is an iconic landmark in the New York skyline, it is competing against newer office towers with large floor plans that are preferred by many tenants. The Chrysler Building was briefly the world's tallest, before it was surpassed by the Empire State Building, which was completed the following year.\n",
    "\"\"\"\n",
    "\n",
    "# Use the endpoint for inference\n",
    "response = predictor.predict({'text': sample_text})\n",
    "print(\"Generated summary:\")\n",
    "print(response['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90958c07",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully trained a BART model on the CNN/DailyMail dataset for text summarization and deployed it to a SageMaker endpoint. This endpoint is now ready to be accessed by our Lambda function to provide summaries for the React frontend.\n",
    "\n",
    "### Important Notes\n",
    "1. We adapted the notebook to work with the Python 3 (ipykernel) kernel available in SageMaker.\n",
    "2. We modified hyperparameters for CPU training (smaller model, smaller batch size, fewer epochs).\n",
    "3. We used only a subset of the data (10%) to make training faster on CPU instances.\n",
    "4. The model quality will be lower than a GPU-trained model with the full dataset, but this serves as a proof of concept.\n",
    "\n",
    "If you want to improve model quality later, consider using a GPU instance type (p3.2xlarge) and adjusting the hyperparameters back to the original values."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
