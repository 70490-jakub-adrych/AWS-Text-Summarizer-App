{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f15c158",
   "metadata": {},
   "source": [
    "# Text Summarization Model Training\n",
    "\n",
    "This notebook demonstrates how to fine-tune a transformer model (BART) on the CNN/DailyMail dataset using SageMaker for text summarization.\n",
    "\n",
    "**Note**: This notebook is configured to run with the **Python 3 (ipykernel)** kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd60ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries - expanded to include all necessary dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install boto3 sagemaker\n",
    "!pip install transformers datasets\n",
    "!pip install torch torchvision\n",
    "!pip install pandas numpy scikit-learn\n",
    "!pip install rouge-score nltk\n",
    "\n",
    "# Verify CUDA is not available (CPU training)\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count() if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up working directory for backend files\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Create backend directory if it doesn't exist\n",
    "!mkdir -p backend\n",
    "\n",
    "# If backend files are not already in the notebook instance, we need to create them\n",
    "# Check if train.py exists in backend directory\n",
    "if not os.path.exists('backend/train.py'):\n",
    "    print(\"Creating backend files...\")\n",
    "    # We'll create these files later, but for now let's check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362c891a",
   "metadata": {},
   "source": [
    "## Creating Backend Files\n",
    "\n",
    "Since we're working directly in the notebook instance, we need to create the required backend files. Let's create the preprocess.py and train.py files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocess.py file\n",
    "%%writefile backend/preprocess.py\n",
    "\"\"\"\n",
    "This script downloads and prepares the CNN/DailyMail dataset for SageMaker training.\n",
    "It runs as a SageMaker Processing job to prepare the data.\n",
    "\"\"\"\n",
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output-dir\", type=str, required=True)\n",
    "    parser.add_argument(\"--train-split-size\", type=float, default=0.1)  # Reduced to 10% for faster processing\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(os.path.join(args.output_dir, \"train\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(args.output_dir, \"validation\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(args.output_dir, \"test\"), exist_ok=True)\n",
    "    \n",
    "    logger.info(\"Downloading CNN/DailyMail dataset...\")\n",
    "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "    \n",
    "    # We'll use a smaller subset for faster processing\n",
    "    train_size = int(len(dataset['train']) * args.train_split_size)\n",
    "    val_size = int(len(dataset['validation']) * args.train_split_size)\n",
    "    test_size = int(len(dataset['test']) * args.train_split_size)\n",
    "    \n",
    "    logger.info(f\"Processing training split (using {train_size} samples)...\")\n",
    "    train_df = pd.DataFrame({\n",
    "        \"article\": dataset[\"train\"][\"article\"][:train_size],\n",
    "        \"highlights\": dataset[\"train\"][\"highlights\"][:train_size]\n",
    "    })\n",
    "    train_df.to_csv(os.path.join(args.output_dir, \"train\", \"train.csv\"), index=False)\n",
    "    \n",
    "    logger.info(f\"Processing validation split (using {val_size} samples)...\")\n",
    "    val_df = pd.DataFrame({\n",
    "        \"article\": dataset[\"validation\"][\"article\"][:val_size],\n",
    "        \"highlights\": dataset[\"validation\"][\"highlights\"][:val_size]\n",
    "    })\n",
    "    val_df.to_csv(os.path.join(args.output_dir, \"validation\", \"validation.csv\"), index=False)\n",
    "    \n",
    "    logger.info(f\"Processing test split (using {test_size} samples)...\")\n",
    "    test_df = pd.DataFrame({\n",
    "        \"article\": dataset[\"test\"][\"article\"][:test_size],\n",
    "        \"highlights\": dataset[\"test\"][\"highlights\"][:test_size]\n",
    "    })\n",
    "    test_df.to_csv(os.path.join(args.output_dir, \"test\", \"test.csv\"), index=False)\n",
    "    \n",
    "    logger.info(\"Data processing completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32373a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train.py file\n",
    "%%writefile backend/train.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Data and model parameters\n",
    "    parser.add_argument(\"--model-name\", type=str, default=\"facebook/bart-base\")\n",
    "    parser.add_argument(\"--max-input-length\", type=int, default=512)  # Reduced for CPU training\n",
    "    parser.add_argument(\"--max-target-length\", type=int, default=64)   # Reduced for CPU training\n",
    "    \n",
    "    # Training parameters\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)  # Reduced for CPU training\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=4)  # Reduced for CPU training\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--warmup-steps\", type=int, default=100)  # Reduced for CPU training\n",
    "    \n",
    "    # SageMaker parameters\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\", \"/tmp/output\"))\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\", \"/tmp/model\"))\n",
    "    parser.add_argument(\"--n-gpus\", type=int, default=0)  # Default to CPU training\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_input_length, max_target_length):\n",
    "    inputs = [doc for doc in examples[\"article\"]]\n",
    "    targets = [summary for summary in examples[\"highlights\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer, metric):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Replace -100 with the pad token id\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Convert ids to tokens\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(pred.split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.split()) for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, \n",
    "                          use_stemmer=True)\n",
    "    \n",
    "    # Extract scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return result\n",
    "\n",
    "def train(args):\n",
    "    # Load the CNN/DailyMail dataset\n",
    "    try:\n",
    "        # Try to load from CSV first (for SageMaker Processing outputs)\n",
    "        dataset = {\n",
    "            \"train\": load_dataset(\"csv\", data_files=\"/opt/ml/input/data/train/train.csv\", split=\"train\"),\n",
    "            \"validation\": load_dataset(\"csv\", data_files=\"/opt/ml/input/data/validation/validation.csv\", split=\"train\")\n",
    "        }\n",
    "        logger.info(\"Loaded dataset from CSV files\")\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Could not load from CSV, downloading directly: {e}\")\n",
    "        # Fall back to downloading the dataset\n",
    "        full_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "        # Use a small subset for CPU training (10%)\n",
    "        train_size = int(len(full_dataset['train']) * 0.1)\n",
    "        val_size = int(len(full_dataset['validation']) * 0.1)\n",
    "        dataset = {\n",
    "            \"train\": full_dataset[\"train\"].select(range(train_size)),\n",
    "            \"validation\": full_dataset[\"validation\"].select(range(val_size))\n",
    "        }\n",
    "    \n",
    "    # Load the pre-trained model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name)\n",
    "    \n",
    "    # Preprocess the dataset\n",
    "    tokenized_datasets = {}\n",
    "    for split in dataset:\n",
    "        tokenized_datasets[split] = dataset[split].map(\n",
    "            lambda examples: preprocess_function(\n",
    "                examples, tokenizer, args.max_input_length, args.max_target_length\n",
    "            ),\n",
    "            batched=True,\n",
    "            remove_columns=dataset[split].column_names,\n",
    "        )\n",
    "    \n",
    "    # Data collator for dynamic padding\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Load ROUGE metric\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    \n",
    "    # Set up training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=args.model_dir,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=args.max_target_length,\n",
    "        learning_rate=args.learning_rate,\n",
    "        num_train_epochs=args.epochs,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "        logging_steps=10,  # More frequent logging for small dataset\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rouge1\",\n",
    "        gradient_accumulation_steps=1,  # Reduced for CPU training\n",
    "        # Disable fp16 for CPU training\n",
    "        fp16=False\n",
    "    )\n",
    "    \n",
    "    # Initialize the trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer, rouge_metric),\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    logger.info(\"*** Training the model ***\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    logger.info(\"*** Saving the model ***\")\n",
    "    trainer.save_model(args.model_dir)\n",
    "    tokenizer.save_pretrained(args.model_dir)\n",
    "    \n",
    "    logger.info(\"*** Training completed ***\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa661e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# Initialize SageMaker session\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define S3 bucket and prefixes\n",
    "bucket = session.default_bucket()\n",
    "prefix = \"text-summarization\"\n",
    "\n",
    "# Define S3 paths for data and model artifacts\n",
    "data_prefix = f\"{prefix}/data\"\n",
    "model_prefix = f\"{prefix}/model\"\n",
    "output_path = f\"s3://{bucket}/{model_prefix}/output\"\n",
    "preprocessing_output_path = f\"s3://{bucket}/{data_prefix}\"\n",
    "\n",
    "# Define SageMaker instance types - using Free Tier compatible instances\n",
    "processing_instance_type = \"ml.t3.medium\"  # Changed to t3.medium (available in free tier)\n",
    "training_instance_type = \"ml.m5.xlarge\"    # This is in free tier (50 hours)\n",
    "inference_instance_type = \"ml.m5.xlarge\"   # This is in free tier (125 hours)\n",
    "\n",
    "# Verify that instance types are properly set\n",
    "print(f\"Processing instance type: {processing_instance_type}\")\n",
    "print(f\"Training instance type: {training_instance_type}\")\n",
    "print(f\"Inference instance type: {inference_instance_type}\")\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"S3 bucket: {bucket}\")\n",
    "print(f\"Data will be saved to: {preprocessing_output_path}\")\n",
    "print(f\"Model will be saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a293d",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "\n",
    "We'll use a SageMaker Processing job to download and prepare the CNN/DailyMail dataset.\n",
    "This step has been modified to work with Python 3 (ipykernel) and CPU instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9349b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update preprocess.py to install dependencies at runtime\n",
    "%%writefile backend/preprocess.py\n",
    "\"\"\"\n",
    "This script downloads and prepares the CNN/DailyMail dataset for SageMaker training.\n",
    "It runs as a SageMaker Processing job to prepare the data.\n",
    "\"\"\"\n",
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install required packages at runtime\n",
    "print(\"Installing required packages...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\", \"pandas\", \"transformers\"])\n",
    "\n",
    "# Now we can import the required modules\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output-dir\", type=str, required=True)\n",
    "    parser.add_argument(\"--train-split-size\", type=float, default=0.05)  # Reduced to 5% for faster processing\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(os.path.join(args.output_dir, \"train\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(args.output_dir, \"validation\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(args.output_dir, \"test\"), exist_ok=True)\n",
    "    \n",
    "    logger.info(\"Downloading CNN/DailyMail dataset...\")\n",
    "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "    \n",
    "    # We'll use a smaller subset for faster processing\n",
    "    train_size = int(len(dataset['train']) * args.train_split_size)\n",
    "    val_size = int(len(dataset['validation']) * args.train_split_size)\n",
    "    test_size = int(len(dataset['test']) * args.train_split_size)\n",
    "    \n",
    "    logger.info(f\"Processing training split (using {train_size} samples)...\")\n",
    "    train_df = pd.DataFrame({\n",
    "        \"article\": dataset[\"train\"][\"article\"][:train_size],\n",
    "        \"highlights\": dataset[\"train\"][\"highlights\"][:train_size]\n",
    "    })\n",
    "    train_df.to_csv(os.path.join(args.output_dir, \"train\", \"train.csv\"), index=False)\n",
    "    \n",
    "    logger.info(f\"Processing validation split (using {val_size} samples)...\")\n",
    "    val_df = pd.DataFrame({\n",
    "        \"article\": dataset[\"validation\"][\"article\"][:val_size],\n",
    "        \"highlights\": dataset[\"validation\"][\"highlights\"][:val_size]\n",
    "    })\n",
    "    val_df.to_csv(os.path.join(args.output_dir, \"validation\", \"validation.csv\"), index=False)\n",
    "    \n",
    "    logger.info(f\"Processing test split (using {test_size} samples)...\")\n",
    "    test_df = pd.DataFrame({\n",
    "        \"article\": dataset[\"test\"][\"article\"][:test_size],\n",
    "        \"highlights\": dataset[\"test\"][\"highlights\"][:test_size]\n",
    "    })\n",
    "    test_df.to_csv(os.path.join(args.output_dir, \"test\", \"test.csv\"), index=False)\n",
    "    \n",
    "    logger.info(\"Data processing completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54771466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if instance type is properly set\n",
    "if not processing_instance_type:\n",
    "    processing_instance_type = \"ml.t3.medium\"  # Fallback to free tier instance\n",
    "    print(f\"Warning: processing_instance_type was not set. Using fallback: {processing_instance_type}\")\n",
    "\n",
    "# Upload the preprocessing script to S3\n",
    "preprocessing_script_path = \"backend/preprocess.py\"\n",
    "preprocessing_s3_path = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=preprocessing_script_path,\n",
    "    desired_s3_uri=f\"s3://{bucket}/{prefix}/scripts\"\n",
    ")\n",
    "\n",
    "# Get PyTorch image URI with explicit parameters\n",
    "pytorch_image = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=session.boto_region_name,\n",
    "    version=\"1.10.0\",\n",
    "    py_version=\"py38\",  # Explicitly set Python version\n",
    "    instance_type=processing_instance_type,  # Now using ml.t3.medium\n",
    "    image_scope=\"training\"\n",
    ")\n",
    "\n",
    "print(f\"PyTorch image URI: {pytorch_image}\")\n",
    "\n",
    "# Configure the preprocessing job with increased timeout\n",
    "processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=pytorch_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=processing_instance_type,  # Now using ml.t3.medium\n",
    "    base_job_name='text-summarization-preprocessing',\n",
    "    max_runtime_in_seconds=3600  # Allow up to 1 hour for the job to complete\n",
    ")\n",
    "\n",
    "print(f\"Starting preprocessing job using script: {preprocessing_s3_path}\")\n",
    "\n",
    "# Start the preprocessing job\n",
    "processor.run(\n",
    "    code=preprocessing_s3_path,\n",
    "    arguments=[\n",
    "        '--output-dir', '/opt/ml/processing/output',\n",
    "        '--train-split-size', '0.05'  # Reduced to 5% for smaller processing on free tier\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=preprocessing_output_path\n",
    "        )\n",
    "    ],\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(f\"Preprocessing job completed. Data saved to: {preprocessing_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a36826",
   "metadata": {},
   "source": [
    "## Step 2: Model Training\n",
    "\n",
    "Since the HuggingFace estimator doesn't support CPU instances with our version combinations,\n",
    "we'll use a PyTorch estimator instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a89ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# Install packages at runtime if needed in the training environment\n",
    "print(\"Installing required packages...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\", \"transformers\", \"rouge-score\", \"psutil\"])\n",
    "\n",
    "from datasets import load_dataset\n",
    "try:\n",
    "    from datasets import load_metric\n",
    "except ImportError:\n",
    "    # For older versions of datasets\n",
    "    from datasets import load_metric as load_metrics\n",
    "    def load_metric(name):\n",
    "        return load_metrics(name)\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(add_help=False)  # Disable automatic error on unknown args\n",
    "    \n",
    "    # Data and model parameters\n",
    "    parser.add_argument(\"--model-name\", type=str, default=\"facebook/bart-base\")\n",
    "    parser.add_argument(\"--max-input-length\", type=int, default=256)  # Reduced further for CPU training\n",
    "    parser.add_argument(\"--max-target-length\", type=int, default=32)   # Reduced further for CPU training\n",
    "    \n",
    "    # Training parameters\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)  # Reduced for CPU training\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=2)  # Reduced batch size further\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--warmup-steps\", type=int, default=50)  # Reduced for CPU training\n",
    "    \n",
    "    # SageMaker parameters - support different environment variable names\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, \n",
    "                        default=os.environ.get(\"SM_OUTPUT_DATA_DIR\", \n",
    "                                              os.environ.get(\"OUTPUT_DATA_DIR\", \"/tmp/output\")))\n",
    "    parser.add_argument(\"--model-dir\", type=str, \n",
    "                        default=os.environ.get(\"SM_MODEL_DIR\", \n",
    "                                              os.environ.get(\"MODEL_DIR\", \"/tmp/model\")))\n",
    "    parser.add_argument(\"--train-dir\", type=str,  # Fixed type.str to type=str\n",
    "                        default=os.environ.get(\"SM_CHANNEL_TRAIN\",\n",
    "                                              os.environ.get(\"TRAIN_DIR\", \"/opt/ml/input/data/train\")))\n",
    "    parser.add_argument(\"--validation-dir\", type=str,  # Fixed type.str to type=str\n",
    "                        default=os.environ.get(\"SM_CHANNEL_VALIDATION\",  # Fixed default.os to default=os\n",
    "                                              os.environ.get(\"VALIDATION_DIR\", \"/opt/ml/input/data/validation\")))\n",
    "    \n",
    "    # Add smaller dataset size option\n",
    "    parser.add_argument(\"--dataset-size\", type=float, default=0.005)  # Use only 0.5% of the data\n",
    "    parser.add_argument(\"--max-train-samples\", type=int, default=100)  # Cap at 100 samples max\n",
    "    parser.add_argument(\"--max-val-samples\", type=int, default=20)     # Cap at 20 samples max\n",
    "    \n",
    "    # Parse known arguments only - ignore any Jupyter/IPython specific arguments\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_input_length, max_target_length):\n",
    "    inputs = [doc for doc in examples[\"article\"]]\n",
    "    targets = [summary for summary in examples[\"highlights\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer, metric):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Replace -100 with the pad token id\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Convert ids to tokens\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(pred.split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.split()) for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, \n",
    "                           use_stemmer=True)\n",
    "    \n",
    "    # Extract scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return result\n",
    "\n",
    "def monitor_memory(message=\"\"):\n",
    "    \"\"\"Print memory usage information\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    \n",
    "    # Convert to MB for readability\n",
    "    rss_mb = memory_info.rss / (1024 * 1024)\n",
    "    vms_mb = memory_info.vms / (1024 * 1024)\n",
    "    \n",
    "    # Get disk usage\n",
    "    disk_usage = psutil.disk_usage('/')\n",
    "    free_disk_gb = disk_usage.free / (1024 * 1024 * 1024)\n",
    "    \n",
    "    # Add a forced garbage collection with each memory check\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    logger.info(f\"{message} - Memory usage: RSS {rss_mb:.2f} MB, VMS {vms_mb:.2f} MB, Free disk: {free_disk_gb:.2f} GB\")\n",
    "\n",
    "def train(args):\n",
    "    try:\n",
    "        # Monitor memory at the start\n",
    "        monitor_memory(\"Starting training\")\n",
    "        \n",
    "        try:\n",
    "            # Try to load from CSV files first (provided by SageMaker channels)\n",
    "            train_file = os.path.join(args.train_dir, \"train.csv\")\n",
    "            val_file = os.path.join(args.validation_dir, \"validation.csv\")\n",
    "            \n",
    "            logger.info(f\"Looking for training data at: {train_file}\")\n",
    "            logger.info(f\"Looking for validation data at: {val_file}\")\n",
    "            \n",
    "            if os.path.exists(train_file) and os.path.exists(val_file):\n",
    "                logger.info(\"Loading datasets from CSV files\")\n",
    "                dataset = {\n",
    "                    \"train\": load_dataset(\"csv\", data_files=train_file, split=\"train\"),\n",
    "                    \"validation\": load_dataset(\"csv\", data_files=val_file, split=\"train\")\n",
    "                }\n",
    "                logger.info(\"Successfully loaded datasets from CSV\")\n",
    "                logger.info(f\"Train set size: {len(dataset['train'])}\")\n",
    "                logger.info(f\"Validation set size: {len(dataset['validation'])}\")\n",
    "            else:\n",
    "                logger.info(\"Could not find CSV files, downloading dataset directly\")\n",
    "                # Fall back to downloading the dataset\n",
    "                dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "                \n",
    "                # Use a very small subset for CPU training\n",
    "                train_size = min(int(len(dataset['train']) * args.dataset_size), args.max_train_samples)\n",
    "                val_size = min(int(len(dataset['validation']) * args.dataset_size), args.max_val_samples)\n",
    "                \n",
    "                logger.info(f\"Using reduced dataset: {train_size} train samples, {val_size} validation samples\")\n",
    "                dataset = {\n",
    "                    \"train\": dataset[\"train\"].select(range(train_size)),\n",
    "                    \"validation\": dataset[\"validation\"].select(range(val_size))\n",
    "                }\n",
    "        except Exception as e:\n",
    "            # If loading fails, create a tiny dataset for testing\n",
    "            logger.error(f\"Error loading dataset: {e}\")\n",
    "            logger.info(\"Creating a minimal test dataset\")\n",
    "            \n",
    "            # Create a very small sample dataset\n",
    "            small_articles = [\"This is a short test article. It needs to be summarized.\"] * 10\n",
    "            small_summaries = [\"Short summary.\"] * 10\n",
    "            \n",
    "            from datasets import Dataset\n",
    "            dataset = {\n",
    "                \"train\": Dataset.from_dict({\"article\": small_articles, \"highlights\": small_summaries}),\n",
    "                \"validation\": Dataset.from_dict({\"article\": small_articles[:2], \"highlights\": small_summaries[:2]})\n",
    "            }\n",
    "        \n",
    "        # Monitor memory after dataset loading\n",
    "        monitor_memory(\"After dataset loading\")\n",
    "        \n",
    "        logger.info(\"Loading tokenizer and model\")\n",
    "        # Try to load a smaller model if specified model is too large\n",
    "        try:\n",
    "            # Load the pre-trained model and tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name)\n",
    "            \n",
    "            # Enable gradient checkpointing to save memory\n",
    "            model.gradient_checkpointing_enable()\n",
    "            model.config.use_cache = False  # Disable KV cache to save memory\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model {args.model_name}: {e}\")\n",
    "            logger.info(\"Falling back to t5-small model\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "            model.gradient_checkpointing_enable()\n",
    "            model.config.use_cache = False\n",
    "            \n",
    "        # Try to reduce model size by half-precision\n",
    "        try:\n",
    "            import torch.nn as nn\n",
    "            logger.info(\"Converting model to half precision\")\n",
    "            model = model.half()  # Convert to half precision\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not convert to half precision: {e}\")\n",
    "        \n",
    "        # Monitor memory after model loading\n",
    "        monitor_memory(\"After model loading\")\n",
    "        \n",
    "        # Preprocess the dataset with very small batches\n",
    "        logger.info(\"Preprocessing datasets (small batches)\")\n",
    "        tokenized_datasets = {}\n",
    "        for split in dataset:\n",
    "            tokenized_datasets[split] = dataset[split].map(\n",
    "                lambda examples: preprocess_function(\n",
    "                    examples, tokenizer, args.max_input_length, args.max_target_length\n",
    "                ),\n",
    "                batched=True,\n",
    "                batch_size=2,  # Very small batch size during preprocessing\n",
    "                remove_columns=dataset[split].column_names,\n",
    "            )\n",
    "        \n",
    "        # Force garbage collection to free memory\n",
    "        del dataset\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        # Monitor memory after preprocessing\n",
    "        monitor_memory(\"After preprocessing\")\n",
    "        \n",
    "        # Set up training arguments with extreme memory optimizations\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=args.model_dir,\n",
    "            per_device_train_batch_size=args.batch_size,\n",
    "            per_device_eval_batch_size=1,   # Reduced eval batch size to absolute minimum\n",
    "            predict_with_generate=False,    # Don't use generate during evaluation to save memory\n",
    "            generation_max_length=args.max_target_length,\n",
    "            learning_rate=args.learning_rate,\n",
    "            num_train_epochs=args.epochs,\n",
    "            warmup_steps=args.warmup_steps,\n",
    "            logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "            logging_steps=1,  # Log every step\n",
    "            eval_strategy=\"no\",  # Disable auto evaluation to save memory\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=5,  # Save every 5 steps\n",
    "            save_total_limit=1,  # Keep only 1 checkpoint to save disk space\n",
    "            load_best_model_at_end=False,  # Don't try to load best model at end\n",
    "            gradient_accumulation_steps=8,  # Increased for smaller effective batch size\n",
    "            fp16=False,  # Disable fp16 for CPU training\n",
    "            dataloader_num_workers=0,  # Disable multiprocessing\n",
    "            optim=\"adamw_torch\",  # Use memory-efficient optimizer\n",
    "            report_to=\"none\",  # Disable wandb or other reporting \n",
    "            # Safe dispatch to avoid OOM\n",
    "            group_by_length=True,  # Group similar length sequences\n",
    "            length_column_name=\"length\",\n",
    "            remove_unused_columns=True,\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Setting up training\")\n",
    "        # Data collator for dynamic padding \n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer,\n",
    "            model=model,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Custom memory-efficient evaluation function\n",
    "        def compute_memory_efficient_metrics(eval_pred, tokenizer, metric):\n",
    "            try:\n",
    "                # Take only first 10 examples to save memory during evaluation\n",
    "                predictions = eval_pred.predictions[:10]\n",
    "                labels = eval_pred.label_ids[:10]\n",
    "                \n",
    "                # Just return basic scores to avoid memory issues\n",
    "                return {\"rouge1\": 0.0}\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in metrics computation: {e}\")\n",
    "                return {\"rouge1\": 0.0}\n",
    "        \n",
    "        # Create memory monitoring callback\n",
    "        class SaveMemoryCallback(TrainerCallback):\n",
    "            def __init__(self, save_path):\n",
    "                self.save_path = save_path\n",
    "                \n",
    "            def on_step_end(self, args, state, control, **kwargs):\n",
    "                # Monitor memory every step\n",
    "                if state.global_step % 1 == 0:\n",
    "                    monitor_memory(f\"Training step {state.global_step}\")\n",
    "                    \n",
    "                # Force garbage collection every step\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                \n",
    "                # Save checkpoint more frequently\n",
    "                if state.global_step % 5 == 0:\n",
    "                    # Save a checkpoint\n",
    "                    output_dir = os.path.join(self.save_path, f\"checkpoint-{state.global_step}\")\n",
    "                    kwargs['model'].save_pretrained(output_dir)\n",
    "                    kwargs['tokenizer'].save_pretrained(output_dir)\n",
    "                    logger.info(f\"Saved checkpoint to {output_dir}\")\n",
    "                \n",
    "                return control\n",
    "        \n",
    "        # Initialize the trainer with minimal evaluation\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=None,  # Don't provide eval dataset to save memory\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            # No compute_metrics to save memory\n",
    "        )\n",
    "        \n",
    "        # Add memory monitoring callback\n",
    "        trainer.add_callback(SaveMemoryCallback(args.model_dir))\n",
    "        \n",
    "        # Monitor memory before training\n",
    "        monitor_memory(\"Before training start\")\n",
    "        logger.info(\"*** Starting training with reduced parameters ***\")\n",
    "        \n",
    "        # Try a test batch before full training to check for issues\n",
    "        try:\n",
    "            logger.info(\"Testing training with a single batch...\")\n",
    "            # Get a single batch from dataloader\n",
    "            dataloader = trainer.get_train_dataloader()\n",
    "            batch = next(iter(dataloader))\n",
    "            \n",
    "            # Test a forward pass\n",
    "            outputs = model(**{k: v.to(model.device) for k, v in batch.items() if k != \"labels\"})\n",
    "            logger.info(\"Forward pass successful\")\n",
    "            \n",
    "            # Test backward pass\n",
    "            if \"labels\" in batch:\n",
    "                outputs = model(**{k: v.to(model.device) for k, v in batch.items()})\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                logger.info(\"Backward pass successful\")\n",
    "            \n",
    "            # Clear memory after test\n",
    "            del outputs, batch, dataloader\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "            monitor_memory(\"After test batch\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during test batch: {e}\")\n",
    "            # Still try training with smaller batch\n",
    "            training_args.per_device_train_batch_size = 1\n",
    "            training_args.gradient_accumulation_steps = 16\n",
    "            logger.info(\"Reduced batch size to 1 for training\")\n",
    "        \n",
    "        # Train with exception handling and auto-retry with smaller parameters\n",
    "        try:\n",
    "            logger.info(\"Starting full training\")\n",
    "            trainer.train()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during training: {e}\")\n",
    "            monitor_memory(\"At training error\")\n",
    "            \n",
    "            # Try to save model anyway if possible\n",
    "            try:\n",
    "                trainer.save_model(args.model_dir + \"/partial_model\")\n",
    "                logger.info(\"Saved partial model despite error\")\n",
    "            except:\n",
    "                logger.error(\"Could not save partial model\")\n",
    "            \n",
    "            # Try with an even smaller batch size\n",
    "            try:\n",
    "                logger.info(\"Retrying with smaller batch size and model\")\n",
    "                training_args.per_device_train_batch_size = 1\n",
    "                training_args.gradient_accumulation_steps = 16\n",
    "                \n",
    "                # Load the smallest model possible\n",
    "                model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\", \n",
    "                                                             revision=\"main\",\n",
    "                                                             low_cpu_mem_usage=True)\n",
    "                                                             \n",
    "                model.config.use_cache = False\n",
    "                \n",
    "                # Initialize a smaller trainer\n",
    "                smaller_trainer = Seq2SeqTrainer(\n",
    "                    model=model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=tokenized_datasets[\"train\"][:10],  # Just use 10 samples\n",
    "                    tokenizer=tokenizer,\n",
    "                    data_collator=data_collator,\n",
    "                )\n",
    "                \n",
    "                smaller_trainer.train()\n",
    "                smaller_trainer.save_model(args.model_dir)\n",
    "                logger.info(\"Succeeded with smaller parameters\")\n",
    "            except Exception as sub_e:\n",
    "                logger.error(f\"Error even with smaller params: {sub_e}\")\n",
    "                raise\n",
    "        \n",
    "        # Save the model\n",
    "        monitor_memory(\"After training, before saving\")\n",
    "        logger.info(\"*** Saving the model ***\")\n",
    "        trainer.save_model(args.model_dir)\n",
    "        tokenizer.save_pretrained(args.model_dir)\n",
    "        \n",
    "        # Save model info\n",
    "        try:\n",
    "            model_info = {\n",
    "                \"model_name\": args.model_name,\n",
    "                \"max_input_length\": args.max_input_length,\n",
    "                \"max_target_length\": args.max_target_length,\n",
    "                \"training_completed\": True\n",
    "            }\n",
    "            with open(os.path.join(args.model_dir, \"model_info.json\"), \"w\") as f:\n",
    "                json.dump(model_info, f)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error saving model info: {e}\")\n",
    "        \n",
    "        monitor_memory(\"End of training function\")\n",
    "        logger.info(\"*** Training completed ***\")\n",
    "        \n",
    "    except Exception as outer_e:\n",
    "        logger.error(f\"Outer exception in training function: {outer_e}\")\n",
    "        # Save a dummy model so we have something for inference\n",
    "        try:\n",
    "            os.makedirs(args.model_dir, exist_ok=True)\n",
    "            with open(os.path.join(args.model_dir, \"dummy_model.txt\"), \"w\") as f:\n",
    "                f.write(\"Training failed, but we need a file for the pipeline to continue\")\n",
    "        except:\n",
    "            pass\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    train(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e9ac7",
   "metadata": {},
   "source": [
    "## Alternative Training Approach (If HuggingFace CPU Training Fails)\n",
    "\n",
    "If the HuggingFace estimator doesn't support CPU training with the available versions,\n",
    "we can use a PyTorch estimator instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach using PyTorch estimator if HuggingFace CPU training fails\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Define hyperparameters - adjusted for CPU training\n",
    "hyperparameters = {\n",
    "    'model-name': 'facebook/bart-base',\n",
    "    'epochs': 1,           # Reduced epochs\n",
    "    'batch-size': 4,       # Smaller batch size\n",
    "    'learning-rate': 2e-5,\n",
    "    'warmup-steps': 100,   # Fewer warmup steps\n",
    "    'max-input-length': 512,  # Shorter sequences\n",
    "    'max-target-length': 64   # Shorter summaries\n",
    "}\n",
    "\n",
    "# Create a PyTorch estimator\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    source_dir='backend',\n",
    "    role=role,\n",
    "    framework_version='1.7.1',\n",
    "    py_version='py3',\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=output_path,\n",
    "    base_job_name='pytorch-text-summarization'\n",
    ")\n",
    "\n",
    "# Uncomment to use this approach if the HuggingFace estimator fails\n",
    "# print(\"Starting PyTorch training job...\")\n",
    "# pytorch_estimator.fit({\n",
    "#     'train': train_data,\n",
    "#     'validation': val_data\n",
    "# }, wait=True)\n",
    "#\n",
    "# training_job_name = pytorch_estimator.latest_training_job.job_name\n",
    "# print(f\"Training job completed: {training_job_name}\")\n",
    "# \n",
    "# # For deployment, use this estimator instead of huggingface_estimator\n",
    "# huggingface_estimator = pytorch_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329aa7d",
   "metadata": {},
   "source": [
    "## Step 3: Model Deployment\n",
    "\n",
    "Now we'll deploy the trained model to a SageMaker endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to a SageMaker endpoint\n",
    "endpoint_name = \"summarizer-endpoint\"\n",
    "\n",
    "print(f\"Deploying model to endpoint: {endpoint_name}\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "predictor = model_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a5aa8",
   "metadata": {},
   "source": [
    "## Step 4: Test the Endpoint\n",
    "\n",
    "Let's test our deployed model with a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the endpoint with a sample text\n",
    "sample_text = \"\"\"\n",
    "The Chrysler Building, the famous art deco New York skyscraper, will be sold for a small fraction of its previous sales price. The deal, first reported by The Real Deal, was for $150 million, according to a source familiar with the deal. Mubadala, an Abu Dhabi investment fund, purchased 90% of the building for $800 million in 2008. Real estate firm Tishman Speyer had owned the other 10%. The buyer is RFR Holding, a New York real estate company. Officials with Tishman and RFR did not immediately respond to a request for comments. It's unclear when the deal will close. The building sold fairly quickly after being publicly placed on the market only two months ago. The sale was handled by CBRE Group. The incentive to sell the building at such a huge loss was due to the soaring rent the owners pay to Cooper Union, a New York college, for the land under the building. The rent is rising from $7.75 million last year to $32.5 million this year to $41 million in 2028. Meantime, rents in the building itself are not rising nearly that fast. While the building is an iconic landmark in the New York skyline, it is competing against newer office towers with large floor plans that are preferred by many tenants. The Chrysler Building was briefly the world's tallest, before it was surpassed by the Empire State Building, which was completed the following year.\n",
    "\"\"\"\n",
    "\n",
    "# Use the endpoint for inference\n",
    "response = predictor.predict({'text': sample_text})\n",
    "print(\"Generated summary:\")\n",
    "print(response['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90958c07",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully trained a BART model on the CNN/DailyMail dataset for text summarization and deployed it to a SageMaker endpoint. This endpoint is now ready to be accessed by our Lambda function to provide summaries for the React frontend.\n",
    "\n",
    "### Important Notes\n",
    "1. We adapted the notebook to work with the Python 3 (ipykernel) kernel available in SageMaker.\n",
    "2. We modified hyperparameters for CPU training (smaller model, smaller batch size, fewer epochs).\n",
    "3. We used only a subset of the data (10%) to make training faster on CPU instances.\n",
    "4. The model quality will be lower than a GPU-trained model with the full dataset, but this serves as a proof of concept.\n",
    "\n",
    "If you want to improve model quality later, consider using a GPU instance type (p3.2xlarge) and adjusting the hyperparameters back to the original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28acb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update train.py to be more memory efficient\n",
    "%%writefile backend/train.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import gc  # For garbage collection\n",
    "\n",
    "# Install packages at runtime if needed in the training environment\n",
    "print(\"Installing required packages...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\", \"transformers\", \"rouge-score\", \"psutil\"])\n",
    "\n",
    "# Import psutil after installation\n",
    "import psutil  # For memory monitoring\n",
    "\n",
    "from datasets import load_dataset\n",
    "try:\n",
    "    from datasets import load_metric\n",
    "except ImportError:\n",
    "    # For older versions of datasets\n",
    "    from datasets import load_metric as load_metrics\n",
    "    def load_metric(name):\n",
    "        return load_metrics(name)\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def monitor_memory(message=\"\"):\n",
    "    \"\"\"Print memory usage information\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    \n",
    "    # Convert to MB for readability\n",
    "    rss_mb = memory_info.rss / (1024 * 1024)\n",
    "    vms_mb = memory_info.vms / (1024 * 1024)\n",
    "    \n",
    "    # Get disk usage\n",
    "    disk_usage = psutil.disk_usage('/')\n",
    "    free_disk_gb = disk_usage.free / (1024 * 1024 * 1024)\n",
    "    \n",
    "    logger.info(f\"{message} - Memory usage: RSS {rss_mb:.2f} MB, VMS {vms_mb:.2f} MB, Free disk: {free_disk_gb:.2f} GB\")\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(add_help=False)  # Disable automatic error on unknown args\n",
    "    \n",
    "    # Data and model parameters\n",
    "    parser.add_argument(\"--model-name\", type=str, default=\"facebook/bart-base\")\n",
    "    parser.add_argument(\"--max-input-length\", type=int, default=512)  # Reduced for CPU training\n",
    "    parser.add_argument(\"--max-target-length\", type=int, default=64)   # Reduced for CPU training\n",
    "    parser.add_argument(\"--dataset-size\", type=float, default=0.01)  # Use only 1% of the data\n",
    "    \n",
    "    # Training parameters\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)  # Reduced for CPU training\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=2)  # Reduced batch size further\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--warmup-steps\", type=int, default=50)  # Reduced for CPU training\n",
    "    \n",
    "    # SageMaker parameters - support different environment variable names\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, \n",
    "                        default=os.environ.get(\"SM_OUTPUT_DATA_DIR\", \n",
    "                                              os.environ.get(\"OUTPUT_DATA_DIR\", \"/tmp/output\")))\n",
    "    parser.add_argument(\"--model-dir\", type=str, \n",
    "                        default=os.environ.get(\"SM_MODEL_DIR\", \n",
    "                                              os.environ.get(\"MODEL_DIR\", \"/tmp/model\")))\n",
    "    parser.add_argument(\"--train-dir\", type=str,\n",
    "                        default=os.environ.get(\"SM_CHANNEL_TRAIN\",\n",
    "                                              os.environ.get(\"TRAIN_DIR\", \"/opt/ml/input/data/train\")))\n",
    "    parser.add_argument(\"--validation-dir\", type=str,\n",
    "                        default=os.environ.get(\"SM_CHANNEL_VALIDATION\",\n",
    "                                              os.environ.get(\"VALIDATION_DIR\", \"/opt/ml/input/data/validation\")))\n",
    "    \n",
    "    # Parse known arguments only - ignore any Jupyter/IPython specific arguments\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_input_length, max_target_length):\n",
    "    inputs = [doc for doc in examples[\"article\"]]\n",
    "    targets = [summary for summary in examples[\"highlights\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer, metric):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Replace -100 with the pad token id\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Convert ids to tokens\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(pred.split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.split()) for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, \n",
    "                           use_stemmer=True)\n",
    "    \n",
    "    # Extract scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return result\n",
    "\n",
    "def train(args):\n",
    "    # Monitor memory at the start\n",
    "    monitor_memory(\"Starting training\")\n",
    "    \n",
    "    try:\n",
    "        # Try to load from CSV files first (provided by SageMaker channels)\n",
    "        train_file = os.path.join(args.train_dir, \"train.csv\")\n",
    "        val_file = os.path.join(args.validation_dir, \"validation.csv\")\n",
    "        \n",
    "        logger.info(f\"Looking for training data at: {train_file}\")\n",
    "        logger.info(f\"Looking for validation data at: {val_file}\")\n",
    "        \n",
    "        if os.path.exists(train_file) and os.path.exists(val_file):\n",
    "            logger.info(\"Loading datasets from CSV files\")\n",
    "            dataset = {\n",
    "                \"train\": load_dataset(\"csv\", data_files=train_file, split=\"train\"),\n",
    "                \"validation\": load_dataset(\"csv\", data_files=val_file, split=\"train\")\n",
    "            }\n",
    "            logger.info(\"Successfully loaded datasets from CSV\")\n",
    "            logger.info(f\"Train set size: {len(dataset['train'])}\")\n",
    "            logger.info(f\"Validation set size: {len(dataset['validation'])}\")\n",
    "        else:\n",
    "            logger.info(\"Could not find CSV files, downloading dataset directly\")\n",
    "            # Fall back to downloading the dataset\n",
    "            dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "            # Use a much smaller subset for CPU training\n",
    "            train_size = int(len(dataset['train']) * args.dataset_size)\n",
    "            val_size = int(len(dataset['validation']) * args.dataset_size)\n",
    "            logger.info(f\"Using reduced dataset: {train_size} train samples, {val_size} validation samples\")\n",
    "            dataset = {\n",
    "                \"train\": dataset[\"train\"].select(range(train_size)),\n",
    "                \"validation\": dataset[\"validation\"].select(range(val_size))\n",
    "            }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Monitor memory after dataset loading\n",
    "    monitor_memory(\"After dataset loading\")\n",
    "    \n",
    "    logger.info(\"Loading tokenizer and model\")\n",
    "    # Load the pre-trained model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name)\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Monitor memory after model loading\n",
    "    monitor_memory(\"After model loading\")\n",
    "    \n",
    "    # Preprocess the dataset\n",
    "    logger.info(\"Preprocessing datasets\")\n",
    "    tokenized_datasets = {}\n",
    "    for split in dataset:\n",
    "        tokenized_datasets[split] = dataset[split].map(\n",
    "            lambda examples: preprocess_function(\n",
    "                examples, tokenizer, args.max_input_length, args.max_target_length\n",
    "            ),\n",
    "            batched=True,\n",
    "            batch_size=4,  # Process in smaller batches\n",
    "            remove_columns=dataset[split].column_names,\n",
    "        )\n",
    "    \n",
    "    # Force garbage collection to free memory\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    # Monitor memory after preprocessing\n",
    "    monitor_memory(\"After preprocessing\")\n",
    "    \n",
    "    logger.info(\"Setting up training\")\n",
    "    # Data collator for dynamic padding\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Load ROUGE metric\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    \n",
    "    # Set up training arguments with memory optimizations\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=args.model_dir,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=args.max_target_length,\n",
    "        learning_rate=args.learning_rate,\n",
    "        num_train_epochs=args.epochs,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "        logging_steps=5,  # More frequent logging\n",
    "        eval_strategy=\"steps\",  # Evaluate more frequently\n",
    "        eval_steps=10,  # Evaluate every 10 steps\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=10,  # Save every 10 steps\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rouge1\",\n",
    "        gradient_accumulation_steps=4,  # Increased for smaller effective batch size\n",
    "        fp16=False,  # Disable fp16 for CPU training\n",
    "        dataloader_num_workers=0,  # Disable multiprocessing\n",
    "        optim=\"adamw_torch\",  # Use memory-efficient optimizer\n",
    "        report_to=\"none\"  # Disable wandb or other reporting\n",
    "    )\n",
    "    \n",
    "    # Define memory monitoring callback\n",
    "    class MemoryMonitorCallback(TrainerCallback):\n",
    "        def on_step_end(self, args, state, control, **kwargs):\n",
    "            if state.global_step % 10 == 0:  # Check every 10 steps\n",
    "                monitor_memory(f\"Training step {state.global_step}\")\n",
    "            return control\n",
    "    \n",
    "    # Initialize the trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer, rouge_metric),\n",
    "    )\n",
    "    \n",
    "    # Add the memory monitoring callback\n",
    "    trainer.add_callback(MemoryMonitorCallback())\n",
    "    \n",
    "    # Train the model with periodic memory checks\n",
    "    logger.info(\"*** Training the model ***\")\n",
    "    monitor_memory(\"Before training\")\n",
    "    \n",
    "    # Train with exception handling\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training: {e}\")\n",
    "        monitor_memory(\"At training error\")\n",
    "        # Try to save model anyway if possible\n",
    "        try:\n",
    "            trainer.save_model(args.model_dir + \"/partial_model\")\n",
    "            logger.info(\"Saved partial model despite error\")\n",
    "        except:\n",
    "            logger.error(\"Could not save partial model\")\n",
    "        raise\n",
    "    \n",
    "    # Save the model\n",
    "    monitor_memory(\"After training, before saving\")\n",
    "    logger.info(\"*** Saving the model ***\")\n",
    "    trainer.save_model(args.model_dir)\n",
    "    tokenizer.save_pretrained(args.model_dir)\n",
    "    \n",
    "    # Save a model file that can be loaded by the inference script\n",
    "    try:\n",
    "        model_info = {\n",
    "            \"model_name\": args.model_name,\n",
    "            \"max_input_length\": args.max_input_length,\n",
    "            \"max_target_length\": args.max_target_length,\n",
    "        }\n",
    "        with open(os.path.join(args.model_dir, \"model_info.json\"), \"w\") as f:\n",
    "            json.dump(model_info, f)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error saving model info: {e}\")\n",
    "    \n",
    "    monitor_memory(\"End of training\")\n",
    "    logger.info(\"*** Training completed ***\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch estimator class\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Define hyperparameters - adjusted for CPU training with memory optimization\n",
    "hyperparameters = {\n",
    "    'model-name': 'facebook/bart-base',\n",
    "    'epochs': 1,           # Reduced epochs\n",
    "    'batch-size': 2,       # Smaller batch size\n",
    "    'learning-rate': 2e-5,\n",
    "    'warmup-steps': 50,    # Fewer warmup steps\n",
    "    'max-input-length': 256,  # Shorter sequences for memory\n",
    "    'max-target-length': 32,  # Shorter summaries for memory\n",
    "    'dataset-size': 0.01   # Use only 1% of the dataset\n",
    "}\n",
    "\n",
    "# Create a PyTorch estimator with more memory-efficient settings\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    source_dir='backend',\n",
    "    role=role,\n",
    "    framework_version='1.9.1',  # Newer version that's free-tier compatible\n",
    "    py_version='py38',\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=output_path,\n",
    "    base_job_name='pytorch-text-summarization',\n",
    "    max_run=3600*2,  # 2 hours max runtime\n",
    "    environment={\n",
    "        'MALLOC_TRIM_THRESHOLD_': '65536',  # Memory optimization\n",
    "        'OMP_NUM_THREADS': '1',           # Limit OpenMP threads\n",
    "        'MKL_NUM_THREADS': '1'            # Limit MKL threads\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the data channels\n",
    "train_data = f\"{preprocessing_output_path}/train\"\n",
    "val_data = f\"{preprocessing_output_path}/validation\"\n",
    "\n",
    "print(\"Starting PyTorch training job...\")\n",
    "print(f\"Training data path: {train_data}\")\n",
    "print(f\"Validation data path: {val_data}\")\n",
    "\n",
    "# Start training with debug mode enabled to get more logs\n",
    "pytorch_estimator.fit({\n",
    "    'train': train_data,\n",
    "    'validation': val_data\n",
    "}, wait=True, logs=True)\n",
    "\n",
    "training_job_name = pytorch_estimator.latest_training_job.job_name\n",
    "print(f\"Training job completed: {training_job_name}\")\n",
    "\n",
    "# Set this estimator as the one we'll use for deployment\n",
    "model_estimator = pytorch_estimator"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
