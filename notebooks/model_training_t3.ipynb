 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization Model Training\n",
    "\n",
    "This notebook demonstrates how to fine-tune a transformer model (BART) on the CNN/DailyMail dataset using SageMaker for text summarization.\n",
    "\n",
    "**Note**: This notebook is configured to run with the **Python 3 (ipykernel)** kernel and is optimized for AWS free tier usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install --upgrade pip\n",
    "!pip install boto3 sagemaker\n",
    "!pip install transformers datasets\n",
    "!pip install torch torchvision\n",
    "!pip install pandas numpy scikit-learn\n",
    "!pip install rouge-score nltk\n",
    "\n",
    "# Verify CUDA is not available (CPU training)\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count() if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for backend directory and files\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if our backend directory exists\n",
    "if not os.path.exists('backend'):\n",
    "    print(\"Creating backend directory...\")\n",
    "    os.makedirs('backend', exist_ok=True)\n",
    "\n",
    "# Print existing files in backend directory to confirm they're there\n",
    "if os.path.exists('backend'):\n",
    "    print(\"Files in backend directory:\")\n",
    "    for file in os.listdir('backend'):\n",
    "        print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Initialize SageMaker session\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define S3 bucket and prefixes\n",
    "bucket = session.default_bucket()\n",
    "prefix = \"text-summarization\"\n",
    "\n",
    "# Define S3 paths for data and model artifacts\n",
    "data_prefix = f\"{prefix}/data\"\n",
    "model_prefix = f\"{prefix}/model\"\n",
    "output_path = f\"s3://{bucket}/{model_prefix}/output\"\n",
    "preprocessing_output_path = f\"s3://{bucket}/{data_prefix}\"\n",
    "\n",
    "# Define SageMaker instance types - using t3.medium which is more commonly available\n",
    "processing_instance_type = \"ml.t3.medium\"  # More commonly available\n",
    "training_instance_type = \"ml.t3.medium\"    # More commonly available\n",
    "inference_instance_type = \"ml.t3.medium\"   # More commonly available\n",
    "\n",
    "# Verify that instance types are properly set\n",
    "print(f\"Processing instance type: {processing_instance_type}\")\n",
    "print(f\"Training instance type: {training_instance_type}\")\n",
    "print(f\"Inference instance type: {inference_instance_type}\")\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"S3 bucket: {bucket}\")\n",
    "print(f\"Data will be saved to: {preprocessing_output_path}\")\n",
    "print(f\"Model will be saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "\n",
    "We'll use a SageMaker Processing job to download and prepare the CNN/DailyMail dataset.\n",
    "This step has been optimized for CPU instances and minimal data usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the preprocessing script to S3\n",
    "preprocessing_script_path = \"backend/preprocess.py\"\n",
    "preprocessing_s3_path = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=preprocessing_script_path,\n",
    "    desired_s3_uri=f\"s3://{bucket}/{prefix}/scripts\"\n",
    ")\n",
    "\n",
    "# Get PyTorch image URI\n",
    "pytorch_image = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=session.boto_region_name,\n",
    "    version=\"1.10.0\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=processing_instance_type,\n",
    "    image_scope=\"training\"\n",
    ")\n",
    "\n",
    "print(f\"PyTorch image URI: {pytorch_image}\")\n",
    "\n",
    "# Configure the preprocessing job\n",
    "processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=pytorch_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=processing_instance_type,\n",
    "    base_job_name='text-summarization-preprocessing',\n",
    "    max_runtime_in_seconds=1800  # 30 minutes max\n",
    ")\n",
    "\n",
    "print(f\"Starting preprocessing job using script: {preprocessing_s3_path}\")\n",
    "\n",
    "# Start the preprocessing job\n",
    "processor.run(\n",
    "    code=preprocessing_s3_path,\n",
    "    arguments=[\n",
    "        '--output-dir', '/opt/ml/processing/output',\n",
    "        '--train-split-size', '0.01',  # Use only 1% of the data\n",
    "        '--max-samples', '100'  # Cap at 100 samples\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=preprocessing_output_path\n",
    "        )\n",
    "    ],\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(f\"Preprocessing job completed. Data saved to: {preprocessing_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Training\n",
    "\n",
    "We'll now use the optimized training script for the model training. The script has been enhanced with:\n",
    "1. Memory-efficient training\n",
    "2. Minimal dataset usage\n",
    "3. Early stopping\n",
    "4. Resource optimization for AWS free tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the training script to S3\n",
    "training_script_path = \"backend/train.py\"\n",
    "training_s3_path = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=training_script_path,\n",
    "    desired_s3_uri=f\"s3://{bucket}/{prefix}/scripts\"\n",
    ")\n",
    "\n",
    "# Define hyperparameters - optimized for free tier\n",
    "hyperparameters = {\n",
    "    'model-name': 'facebook/bart-base',\n",
    "    'epochs': 1,\n",
    "    'batch-size': 1,  # Minimal batch size for memory efficiency\n",
    "    'learning-rate': 2e-5,\n",
    "    'warmup-steps': 10,  # Reduced for faster training\n",
    "    'max-input-length': 128,  # Shorter sequences for memory\n",
    "    'max-target-length': 32,  # Shorter summaries for memory\n",
    "    'dataset-size': 0.01,  # Use only 1% of the dataset\n",
    "    'max-train-samples': 50,  # Cap training samples\n",
    "    'max-val-samples': 10,  # Cap validation samples\n",
    "    'max-steps': 10,  # Very small number of steps\n",
    "    'use-max-steps': True  # Use steps instead of epochs\n",
    "}\n",
    "\n",
    "# Create PyTorch estimator\n",
    "model_estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    source_dir='backend',\n",
    "    role=role,\n",
    "    framework_version='1.10.0',\n",
    "    py_version='py38',\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=output_path,\n",
    "    max_runtime_in_seconds=1800,  # 30 minutes max\n",
    "    base_job_name='text-summarization-training'\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training job...\")\n",
    "try:\n",
    "    model_estimator.fit(\n",
    "        inputs={\n",
    "            'train': preprocessing_output_path + '/train',\n",
    "            'validation': preprocessing_output_path + '/validation'\n",
    "        },\n",
    "        wait=True\n",
    "    )\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Model artifacts saved to: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Check your AWS account limits in the AWS Console\")\n",
    "    print(\"2. Verify that you have sufficient permissions\")\n",
    "    print(\"3. Check if the preprocessing job completed successfully\")\n",
    "    print(\"4. Try using a different instance type\")\n",
    "    print(\"\\nYou can also try training manually through the AWS Console\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Deployment\n",
    "\n",
    "Now we'll deploy the trained model to a SageMaker endpoint for real-time inference.\n",
    "We're using t3.medium instances which are commonly available across regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to a SageMaker endpoint\n",
    "endpoint_name = \"summarizer-endpoint\"\n",
    "\n",
    "print(f\"Deploying model to endpoint: {endpoint_name}\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "try:\n",
    "    # First try with t3.medium\n",
    "    predictor = model_estimator.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=inference_instance_type,  # ml.t3.medium\n",
    "        endpoint_name=endpoint_name,\n",
    "        wait=True\n",
    "    )\n",
    "    print(f\"Model deployed to endpoint: {endpoint_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deploying with t3.medium: {e}\")\n",
    "    print(\"Trying with t3.large...\")\n",
    "    try:\n",
    "        # Try with t3.large as fallback\n",
    "        predictor = model_estimator.deploy(\n",
    "            initial_instance_count=1,\n",
    "            instance_type=\"ml.t3.large\",\n",
    "            endpoint_name=endpoint_name,\n",
    "            wait=True\n",
    "        )\n",
    "        print(f\"Model deployed to endpoint with t3.large: {endpoint_name}\")\n",
    "    except Exception as inner_e:\n",
    "        print(f\"Error deploying with t3.large: {inner_e}\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Check your AWS account limits in the AWS Console\")\n",
    "        print(\"2. Verify that you have sufficient permissions\")\n",
    "        print(\"3. Check if the model artifacts were saved correctly\")\n",
    "        print(\"4. Try deleting any existing endpoints with the same name\")\n",
    "        print(\"\\nYou can also try deploying manually through the AWS Console\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the Endpoint\n",
    "\n",
    "Let's test our deployed model with a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the endpoint with a sample text\n",
    "sample_text = \"\"\"\n",
    "The Chrysler Building, the famous art deco New York skyscraper, will be sold for a small fraction of its previous sales price. The deal, first reported by The Real Deal, was for $150 million, according to a source familiar with the deal. Mubadala, an Abu Dhabi investment fund, purchased 90% of the building for $800 million in 2008. Real estate firm Tishman Speyer had owned the other 10%. The buyer is RFR Holding, a New York real estate company. Officials with Tishman and RFR did not immediately respond to a request for comments. It's unclear when the deal will close. The building sold fairly quickly after being publicly placed on the market only two months ago. The sale was handled by CBRE Group. The incentive to sell the building at such a huge loss was due to the soaring rent the owners pay to Cooper Union, a New York college, for the land under the building. The rent is rising from $7.75 million last year to $32.5 million this year to $41 million in 2028. Meantime, rents in the building itself are not rising nearly that fast. While the building is an iconic landmark in the New York skyline, it is competing against newer office towers with large floor plans that are preferred by many tenants. The Chrysler Building was briefly the world's tallest, before it was surpassed by the Empire State Building, which was completed the following year.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Use the endpoint for inference\n",
    "    response = predictor.predict({'text': sample_text})\n",
    "    print(\"Generated summary:\")\n",
    "    print(response['summary'])\n",
    "except Exception as e:\n",
    "    print(f\"Error during inference: {e}\")\n",
    "    print(\"This could be due to an issue with the endpoint setup or the model itself.\")\n",
    "    print(\"Check the CloudWatch logs for the endpoint for more details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've implemented a memory-efficient training pipeline optimized for AWS with the following improvements:\n",
    "\n",
    "1. **Resource Usage**: Using t3.medium instances which are commonly available across regions\n",
    "2. **Small Dataset**: Using only 1% of the data with a maximum of 100 samples\n",
    "3. **Memory Optimization**: Implemented gradient checkpointing and half-precision training\n",
    "4. **Early Stopping**: Training stops after 10 steps to avoid excessive resource usage\n",
    "5. **Efficient Preprocessing**: Optimized data preprocessing for minimal memory usage\n",
    "6. **Resource Monitoring**: Added memory usage tracking throughout the pipeline\n",
    "\n",
    "The model is now trained and deployed in a way that should work within AWS limits. Note that the model's performance might be limited due to the small dataset and short training time, but it provides a good starting point for experimentation.\n",
    "\n",
    "To improve the model's performance, you could:\n",
    "1. Use a larger dataset (but this would require more resources)\n",
    "2. Train for more steps (but this would increase costs)\n",
    "3. Use a larger model (but this would require more memory)\n",
    "\n",
    "Remember to delete the endpoint when you're done to avoid incurring charges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up - delete the endpoint when done\n",
    "try:\n",
    "    predictor.delete_endpoint()\n",
    "    print(f\"Endpoint {endpoint_name} deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting endpoint: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}